# -*- coding: utf-8 -*-
"""Michael_Thesis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1onHW0lK2Ab6Yv4JO7UdxvQBEKcLO-0Zl
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import shap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import RandomForestRegressor, StackingRegressor
from sklearn.ensemble import VotingRegressor
from sklearn.linear_model import Ridge
from lightgbm import LGBMRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.inspection import permutation_importance

data_path = "/content/drive/MyDrive/New CBS"
os.listdir(data_path)

#read and preview first dataframe from google drive

housing_df = pd.read_csv(os.path.join(data_path, 'Housing & Real Estate.csv'), sep=";", index_col="ID")
housing_df.head()

housing_df.shape

# rename columns

new_column_names = {
    'WijkenEnBuurten': 'Neighborhoods',
    'Codering_3': 'Coding',
    'IndelingswijzigingGemeenteWijkBuurt_4': 'NeighborhoodChanges',
    'Woningvoorraad_35': 'HousingStock',
    'NieuwbouwWoningen_36': 'NewHome',
    'GemiddeldeWOZWaardeVanWoningen_39': 'MeanWOZValueHome',
    'PercentageEengezinswoning_40': '%SingleFamilyHome',
    'PercentageTussenwoningEengezins_41': '%SemiDetached_SingleFamilyHome',
    'PercentageHoekwoningEengezins_42': '%CornerHouseSingleFamily',
    'PercentageTweeOnderEenKapWoningEe_43': '%TwoUnderOneRoofHouseSingleFamily',
    'PercentageVrijstaandeWoningEengezins_44': '%DetachedHouseSingleFamilyHome',
    'PercentageMeergezinswoning_45': '%MultiFamilyHome',
    'OnbewoondeWoningen_46': 'UninhabitedHome',
    'Koopwoningen_47': 'OwnerOccupiedHome',
    'HuurwoningenTotaal_48': 'RentalHouseTotal',
    'InBezitWoningcorporatie_49': 'OwnedByHouseCorporation',
    'InBezitOverigeVerhuurders_50': 'OtherOwners',
    'BouwjaarAfgelopenTienJaar_52': 'BuiltLastTenYears',
    'MeestVoorkomendePostcode_125': 'CommonPostcode',
    'Dekkingspercentage_126': 'Coverage%'
}

housing_df = housing_df.rename(columns=new_column_names)

housing_df.head()

# Strip trailing and leading whitespaces from 'Neighborhoods' and 'Coding' columns
housing_df['Neighborhoods'] = housing_df['Neighborhoods'].str.strip()
housing_df['Coding'] = housing_df['Coding'].str.strip()

# Optional: confirm fix by displaying unique values again
print(housing_df['Neighborhoods'].unique()[:7])
print(housing_df['Coding'].unique()[:7])

housing_df.drop(columns=["NewHome", "Neighborhoods"], inplace=True)

housing_df.columns

housing_df.info()

#copy dataframe
df = housing_df.copy()

# Convert appropriate columns to correct data types
df['Coding'] = df['Coding'].astype(str)

# Columns to convert to numeric (float) ‚Äî remove % symbols if they exist
numeric_cols = [
    'HousingStock', 'MeanWOZValueHome', '%SingleFamilyHome', '%SemiDetached_SingleFamilyHome',
    '%CornerHouseSingleFamily', '%TwoUnderOneRoofHouseSingleFamily',
    '%DetachedHouseSingleFamilyHome', '%MultiFamilyHome', 'UninhabitedHome',
    'OwnerOccupiedHome', 'RentalHouseTotal', 'OwnedByHouseCorporation',
    'OtherOwners', 'BuiltLastTenYears', 'Coverage%'
]

# Remove percentage signs if present and convert
for col in numeric_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce')  # convert to float, NaNs if invalid

# Optional: check conversion
print(df[numeric_cols].dtypes)
df[numeric_cols].head()

df.info()

print("\nMissing values:\n", df.isnull().sum())  # Count missing values

df.shape

df_cleaned = df.dropna(thresh=8)  # Keeps rows with at least 8 non-NA values

print("\nMissing values:\n", df_cleaned.isnull().sum())  # Count missing values

df_cleaned.head()

#  Histogram of WOZ values
plt.hist(df_cleaned['MeanWOZValueHome'], bins=20, edgecolor='black')
plt.title('Distribution of WOZ Home Values')
plt.xlabel('WOZ Value (x ‚Ç¨100k)')
plt.ylabel('Number of Neighborhoods')
plt.xticks(rotation=30, ha='right')
plt.tight_layout()
plt.show()

# Define the housing type columns
housing_type_cols = [
    '%SingleFamilyHome',
    '%SemiDetached_SingleFamilyHome',
    '%CornerHouseSingleFamily',
    '%TwoUnderOneRoofHouseSingleFamily',
    '%DetachedHouseSingleFamilyHome',
    '%MultiFamilyHome'
]

# Calculate the average percentage for each housing type
avg_composition = df_cleaned[housing_type_cols].mean()

# Create the vertical bar plot using matplotlib
plt.figure(figsize=(10, 6))
plt.bar(avg_composition.index, avg_composition.values, color='skyblue', edgecolor='black')

# Customize plot
plt.title('Average Housing Type Composition Across Neighborhoods')
plt.ylabel('Percentage')
plt.xlabel('Housing Type')
plt.xticks(rotation=30, ha='right')
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()

# Show the plot
plt.show()





demographics_df = pd.read_csv(os.path.join(data_path, 'Demographics & Households.csv'), sep=";", index_col="ID")
demographics_df.head()

demographics_df.info()

demographics_df.columns

# Copy to avoid changing the original
df = demographics_df.copy()

# Rename columns for clarity
df.rename(columns={
    'WijkenEnBuurten': 'Neighborhood',
    'SoortRegio_2': 'RegionType',
    'Codering_3': 'Code',
    'IndelingswijzigingGemeenteWijkBuurt_4': 'RegionChange',
    'AantalInwoners_5': 'Population',
    'k_0Tot15Jaar_8': 'Age_0_15',
    'k_15Tot25Jaar_9': 'Age_15_25',
    'k_25Tot45Jaar_10': 'Age_25_45',
    'k_45Tot65Jaar_11': 'Age_45_65',
    'k_65JaarOfOuder_12': 'Age_65plus',
    'HuishoudensTotaal_29': 'TotalHouseholds',
    'Eenpersoonshuishoudens_30': 'SinglePersonHouseholds',
    'HuishoudensZonderKinderen_31': 'HomeWithoutKids',
    'HuishoudensMetKinderen_32': 'HomeWithKids',
    'GemiddeldeHuishoudensgrootte_33': 'AvgHouseholdSize',
    'Bevolkingsdichtheid_34': 'PopulationDensity',
    'MeestVoorkomendePostcode_125': 'CommonPostcode',
    'Dekkingspercentage_126': 'CoveragePercent'
}, inplace=True)

# Strip whitespace from all string/object values
df = df.apply(lambda col: col.map(lambda x: x.strip() if isinstance(x, str) else x))

# Replace any '.' with np.nan
df.replace('.', np.nan, inplace=True)

# Convert numeric columns from object to float
cols_to_convert = ['AvgHouseholdSize', 'PopulationDensity', 'CoveragePercent']
df[cols_to_convert] = df[cols_to_convert].astype(float)

# Show missing values (optional)
print("Successfully converted. Missing values:\n", df[cols_to_convert].isna().sum())

df.info()

# Ensure all object columns are string
object_cols = df.select_dtypes(include='object').columns
df[object_cols] = df[object_cols].astype('string')

# Sort and pick top neighborhoods by population and density
top_pop = df[['Neighborhood', 'Population']].sort_values(by='Population', ascending=False).head(10)
top_density = df[['Neighborhood', 'PopulationDensity']].sort_values(by='PopulationDensity', ascending=False).head(10)

# Plot 1: Top neighborhoods by Population Density
plt.figure(figsize=(10,6))
plt.bar(top_density['Neighborhood'], top_density['PopulationDensity'], color='teal')
plt.xticks(rotation=45, ha='right')
plt.title('Top Neighborhoods by Population Density')
plt.ylabel('Population Density (per km¬≤)')
plt.tight_layout()
plt.show()

# Plot 2: Top neighborhoods by Population (with density as second axis)
fig, ax1 = plt.subplots(figsize=(10,6))

color = 'tab:blue'
ax1.bar(top_pop['Neighborhood'], top_pop['Population'], color=color, label='Population')
ax1.set_ylabel('Population', color=color)
ax1.tick_params(axis='y', labelcolor=color)
ax1.set_xticklabels(top_pop['Neighborhood'], rotation=45, ha='right')

# Create second y-axis for density
ax2 = ax1.twinx()
color = 'tab:red'
pop_density = df.set_index('Neighborhood').loc[top_pop['Neighborhood'], 'PopulationDensity']
ax2.plot(top_pop['Neighborhood'], pop_density, color=color, marker='o', label='Population Density')
ax2.set_ylabel('Population Density (per km¬≤)', color=color)
ax2.tick_params(axis='y', labelcolor=color)

plt.title('Top Neighborhoods by Population and Density')
fig.tight_layout()
plt.show()

# Plot 3: Histogram of Avg Household Size
plt.figure(figsize=(8,5))
df['AvgHouseholdSize'].dropna().plot(kind='hist', bins=15, color='skyblue', edgecolor='black')
plt.title('Distribution of Average Household Size')
plt.xlabel('Average Household Size')
plt.ylabel('Number of Neighborhoods')
plt.tight_layout()
plt.show()

# Ensure coding columns match for merge
housing_df['Coding'] = housing_df['Coding'].str.strip()
df['Code'] = df['Code'].str.strip()

# Merge the dataframes on neighborhood code
merged_df = pd.merge(housing_df, df, left_on='Coding', right_on='Code', how='inner')

# Drop duplicate key columns if necessary
merged_df.drop(columns=['Code', "CommonPostcode_y", "NeighborhoodChanges", "RegionType", "RegionChange", "CoveragePercent"], inplace=True)

# Quick check
print("‚úÖ Merged dataframe shape:", merged_df.shape)
print(merged_df[['Neighborhood', 'Population', 'MeanWOZValueHome']].head())



# Convert MeanWOZValueHome to float
merged_df['MeanWOZValueHome'] = pd.to_numeric(merged_df['MeanWOZValueHome'], errors='coerce')

# Drop rows with missing WOZ values
df_model = merged_df.dropna(subset=['MeanWOZValueHome'])

# Select numerical & relevant features for prediction
features = [
    '%SingleFamilyHome', '%SemiDetached_SingleFamilyHome', '%CornerHouseSingleFamily',
    '%TwoUnderOneRoofHouseSingleFamily', '%DetachedHouseSingleFamilyHome', '%MultiFamilyHome',
    'OwnerOccupiedHome', 'RentalHouseTotal', 'BuiltLastTenYears',
    'Population', 'Age_25_45', 'Age_45_65', 'Age_65plus',
    'TotalHouseholds', 'AvgHouseholdSize', 'PopulationDensity'
]

# Convert features to float
for col in features:
    df_model.loc[:, col] = pd.to_numeric(df_model[col], errors='coerce')


# Drop any rows with remaining NaNs in predictors
df_model = df_model.dropna(subset=features)

# Define X and y
X = df_model[features]
y = df_model['MeanWOZValueHome']

"""## **BASELINE MODEL**
```
# This is formatted as code
```

# Model Without GM0855 and %DetachedHouseSingleFamilyHome
"""

# Remove GM0855 row (Tilburg-wide data)
df_model_reduced = df_model[df_model['Coding'] != 'GM0855'].copy()

# Define reduced feature list (excluding %DetachedHouseSingleFamilyHome)
reduced_features = [
    '%SingleFamilyHome', '%SemiDetached_SingleFamilyHome', '%CornerHouseSingleFamily',
    '%TwoUnderOneRoofHouseSingleFamily', '%MultiFamilyHome',
    'OwnerOccupiedHome', 'RentalHouseTotal', 'BuiltLastTenYears',
    'Population', 'Age_25_45', 'Age_45_65', 'Age_65plus',
    'TotalHouseholds', 'AvgHouseholdSize', 'PopulationDensity'
]

# Convert features to numeric
for col in reduced_features:
    df_model_reduced[col] = pd.to_numeric(df_model_reduced[col], errors='coerce')

# Drop any rows with NaNs
df_model_reduced = df_model_reduced.dropna(subset=reduced_features + ['MeanWOZValueHome'])

# Define X and y
X_reduced = df_model_reduced[reduced_features]
y_reduced = df_model_reduced['MeanWOZValueHome']

# Split data
X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(
    X_reduced, y_reduced, test_size=0.2, random_state=42
)

# Train model
rf_model_r = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model_r.fit(X_train_r, y_train_r)

# Predict
y_pred_r = rf_model_r.predict(X_test_r)

# Evaluate
mae_r = mean_absolute_error(y_test_r, y_pred_r)
rmse_r = np.sqrt(mean_squared_error(y_test_r, y_pred_r))
r2_r = r2_score(y_test_r, y_pred_r)

print("\nüìâ Performance of Baseline Model:")
print(f"MAE: {mae_r:.2f}")
print(f"RMSE: {rmse_r:.2f}")
print(f"R¬≤ Score: {r2_r:.2f}")

# Feature importance plot
importances_r = rf_model_r.feature_importances_
sorted_idx_r = np.argsort(importances_r)[::-1]
sorted_features_r = X_reduced.columns[sorted_idx_r]

plt.figure(figsize=(10, 6))
plt.title("Feature Importances (Without %DetachedHouseSingleFamilyHome & GM0855)")
plt.bar(range(len(importances_r)), importances_r[sorted_idx_r])
plt.xticks(range(len(importances_r)), sorted_features_r, rotation=90)
plt.tight_layout()
plt.show()

# Plot actual vs predicted
plt.figure(figsize=(6, 5))
plt.scatter(y_test_r, y_pred_r, alpha=0.7)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
plt.xlabel("Actual WOZ Value (k‚Ç¨)")
plt.ylabel("Predicted WOZ Value (k‚Ç¨)")
plt.title("Actual vs Predicted WOZ (70/30 Split)")
plt.grid(True)
plt.tight_layout()
plt.show()

# Perform 5-fold cross-validation using neg_mean_squared_error
cv_scores = cross_val_score(
    rf_model_r, X, y,
    cv=5,
    scoring='neg_root_mean_squared_error'
)

# Convert scores to positive RMSE values
cv_rmse_scores = -cv_scores

print("Cross-Validation RMSE Scores:", np.round(cv_rmse_scores, 2))
print(f"Mean CV RMSE: {np.mean(cv_rmse_scores):.2f}")
print(f"Std Dev of CV RMSE: {np.std(cv_rmse_scores):.2f}")

# Setup KFold
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Prepare for loop
fold = 1
for train_idx, test_idx in kf.split(X):
    if fold == 5:  # We only care about Fold 5 here
        # Split data
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
        neighborhoods = df_model.iloc[test_idx]['Neighborhood'].values  # Get neighborhood names

        # Train model
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        # Evaluation
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))

        print(f"üì¶ Fold 5 MAE: {mae:.2f}")
        print(f"üì¶ Fold 5 RMSE: {rmse:.2f}")

        # DataFrame of results
        results = pd.DataFrame({
            'Neighborhood': neighborhoods,
            'Actual': y_test.values,
            'Predicted': y_pred,
            'Error': y_test.values - y_pred,
            'AbsoluteError': np.abs(y_test.values - y_pred)
        }).sort_values(by='AbsoluteError', ascending=False)

        print("\nüìç Top 5 Largest Errors in Fold 5:")
        print(results.head())

        # Plot: Actual vs Predicted
        plt.figure(figsize=(6, 5))
        plt.scatter(results['Actual'], results['Predicted'], alpha=0.7)
        plt.plot([results['Actual'].min(), results['Actual'].max()],
                 [results['Actual'].min(), results['Actual'].max()], 'r--')
        plt.xlabel("Actual WOZ Value (k‚Ç¨)")
        plt.ylabel("Predicted WOZ Value (k‚Ç¨)")
        plt.title("Fold 5: Actual vs Predicted WOZ")
        plt.grid(True)
        plt.tight_layout()
        plt.show()

        # Plot: Absolute Error Bar Chart
        plt.figure(figsize=(10, 5))
        results_sorted = results.sort_values(by='AbsoluteError', ascending=False)
        plt.bar(results_sorted['Neighborhood'], results_sorted['AbsoluteError'])
        plt.xticks(rotation=90)
        plt.ylabel("Absolute Error (k‚Ç¨)")
        plt.title("Absolute Prediction Error by Neighborhood (Fold 5)")
        plt.tight_layout()
        plt.show()

        break  # We only want Fold 5
    fold += 1

df_model[df_model['Neighborhood'].isin(['WK085560', 'BU08553801', 'BU08556614', 'BU08556603', 'BU08556708'])]





# Load and preview energy and sustainability data

energy_df = pd.read_csv(os.path.join(data_path, 'Energy & Sustainability.csv'), sep=";", index_col="ID")
energy_df.head()

energy_df.columns

# Assume df_energy is your sustainability dataset
energy_df = energy_df.rename(columns={
    'WijkenEnBuurten': 'Neighborhood',
    'SoortRegio_2': 'RegionType',
    'Codering_3': 'Coding',
    'GemiddeldeElektriciteitsleveringTotaal_48': 'AvgElectricity',
    'Appartement_49': 'Elec_Apartment',
    'Tussenwoning_50': 'Elec_RowHouse',
    'Hoekwoning_51': 'Elec_CornerHouse',
    'TweeOnderEenKapWoning_52': 'Elec_SemiDetached',
    'VrijstaandeWoning_53': 'Elec_Detached',
    'Huurwoning_54': 'Elec_Rental',
    'EigenWoning_55': 'Elec_Owned',
    'GemiddeldAardgasverbruikTotaal_56': 'AvgGas',
    'Appartement_57': 'Gas_Apartment',
    'Tussenwoning_58': 'Gas_RowHouse',
    'Hoekwoning_59': 'Gas_CornerHouse',
    'TweeOnderEenKapWoning_60': 'Gas_SemiDetached',
    'VrijstaandeWoning_61': 'Gas_Detached',
    'Huurwoning_62': 'Gas_Rental',
    'EigenWoning_63': 'Gas_Owned',
    'PercentageWoningenMetStadsverwarming_64': 'DistrictHeatingPercent',
    'MeestVoorkomendePostcode_120': 'CommonPostcode'
})

# Remove whitespace and convert numeric fields
energy_df = energy_df.applymap(lambda x: str(x).strip() if isinstance(x, str) else x)

# Convert appropriate columns to numeric
num_cols = energy_df.columns.difference(['Neighborhood', 'RegionType', 'Coding', 'CommonPostcode'])
energy_df[num_cols] = energy_df[num_cols].apply(pd.to_numeric, errors='coerce')

# Check for missing values
print(energy_df.isna().sum())

# Quick overview
print(energy_df.info())
energy_df.head()



# Merge energy data with main model on 'Coding'
df_model_energy = pd.merge(
    df_model,
    energy_df,
    how='left',
    left_on='Coding',
    right_on='Coding'
)

print("Merged shape:", df_model_energy.shape)

#Convert DistrictHeatingPercent to binary flag
df_model_energy['HasDistrictHeating'] = df_model_energy['DistrictHeatingPercent'].notnull().astype(int)

df_model_energy.head()

#Drop redundant columns
df_model_energy.info()

# Create new feature: Gas-to-Electricity Ratio
df_model_energy['GasPerElecRatio'] = df_model_energy['AvgGas'] / df_model_energy['AvgElectricity']

# Plot distributions
energy_cols = ['AvgElectricity', 'AvgGas', 'GasPerElecRatio']
plt.figure(figsize=(14, 4))
for i, col in enumerate(energy_cols):
    plt.subplot(1, 3, i+1)
    sns.histplot(df_model_energy[col], bins=30, kde=True, color='teal')
    plt.title(f'{col} Distribution')
plt.tight_layout()
plt.show()

# Correlation with WOZ
corrs = df_model_energy[['MeanWOZValueHome'] + energy_cols + ['HasDistrictHeating']].corr()
print("\nüîç Correlation with WOZ Value:")
print(corrs['MeanWOZValueHome'].sort_values(ascending=False))

# Drop GM0855 (the city-level row)
df_model_energy = df_model_energy[df_model_energy['Coding'] != 'GM0855']

energy_features = [
    'AvgElectricity',       # Strongest correlation
    'AvgGas',               # Reasonable correlation
    'GasPerElecRatio',      # Efficiency / sustainability indicator
    'HasDistrictHeating'    # Urban heating infrastructure
]

# üéØ Step 1: Define refined feature set
selected_features = [
    'AvgElectricity', 'OwnerOccupiedHome', 'RentalHouseTotal',
    'BuiltLastTenYears', '%TwoUnderOneRoofHouseSingleFamily',
    'AvgGas', 'AvgHouseholdSize', '%SemiDetached_SingleFamilyHome', 'Age_65plus'
]

# Low-impact features to drop
low_impact_features = [
    '%SingleFamilyHome', 'Population', '%MultiFamilyHome',
    'TotalHouseholds', '%CornerHouseSingleFamily'
]

# Drop GM0855 row
df_filtered = df_model_energy[df_model_energy['Coding'] != 'GM0855'].copy()

# Define target and features
target = 'MeanWOZValueHome'
features = [
    col for col in df_filtered.columns
    if col not in low_impact_features + ['Coding', target]
]

# Drop non-numeric columns (before imputation)
X = df_filtered[features].select_dtypes(include=[np.number]).copy()
y = pd.to_numeric(df_filtered[target], errors='coerce')

# Drop target NA
valid_idx = y.notna()
X = X.loc[valid_idx]
y = y.loc[valid_idx]

# Impute missing values
imputer = SimpleImputer(strategy='median')
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, random_state=42
)

# Train XGBoost
xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)
xgb_model.fit(X_train, y_train.to_numpy())

# Evaluate
y_pred = xgb_model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\n Cleaned XGBoost Model Performance (with Imputation):")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R¬≤ Score: {r2:.2f}")

# SHAP Summary
explainer = shap.Explainer(xgb_model, X_train)
shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test, plot_type="bar")
shap.summary_plot(shap_values, X_test)





# Load and preview income and Socioeconomic data

income_df = pd.read_csv(os.path.join(data_path, 'Income.csv'), sep=";", index_col="ID")
income_df.head()

income_df.columns

# Define and drop unnecessary columns first
columns_to_drop = [
    "SoortRegio_2", "Nettoarbeidsparticipatie_73",
    "PercentageWerknemers_74", "PercentageZelfstandigen_75"
]

income_df.drop(columns=columns_to_drop, inplace=True, errors="ignore")

#  Rename columns for readability
income_df.columns = [
    "Neighborhood", "Coding", "IncomeReceivers",
    "AvgIncomePerReceiver", "AvgIncomePerResident",
    "PctLowest40Incomes", "PctTop20Incomes", "AvgStandardizedIncome",
    "PctLowest40HouseholdsIncome", "PctTop20HouseholdsIncome",
    "LowIncomeHouseholds", "BelowOrAroundSocialMinimum",
    "Below110PctSocialMin", "Below120PctSocialMin",
    "MedianWealth", "CommonPostcode"
]

# Replace '.' with NaN and convert types
income_df.replace(".", np.nan, inplace=True)
for col in income_df.columns:
    if income_df[col].dtype == "object":
        income_df[col] = income_df[col].map(lambda x: str(x).strip() if isinstance(x, str) else x)

# Convert all numeric-looking columns to floats
for col in income_df.columns[2:]:
    income_df[col] = pd.to_numeric(income_df[col], errors="coerce")

# Preview structure
print("\nüîç Missing values:")
print(income_df.isna().sum())

income_df.info()

# Drop high-missing columns
cols_to_drop = [
    'AvgIncomePerReceiver',
    'AvgIncomePerResident',
    'AvgStandardizedIncome'
]
income_cleaned = income_df.drop(columns=cols_to_drop)

# Impute remaining numeric columns with median
imputer = SimpleImputer(strategy='median')
numeric_cols = income_cleaned.select_dtypes(include=[np.number]).columns
income_cleaned[numeric_cols] = imputer.fit_transform(income_cleaned[numeric_cols])

# üëÄ Final check
print("\n‚úÖ Cleaned income data:")
print(income_cleaned.info())
income_cleaned.head(3)

# Drop Tilburg-wide aggregate
income_cleaned = income_cleaned[income_cleaned['Coding'] != 'GM0855'].copy()

# Plot histograms for all numeric income features
numeric_cols = income_cleaned.select_dtypes(include='number').columns.drop(['CommonPostcode'])

income_cleaned[numeric_cols].hist(
    figsize=(15, 12),
    bins=30,
    edgecolor='black',
    layout=(4, 3)
)
plt.suptitle("Distributions of Income & Socioeconomic Features", fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()







#  Merge df_filtered and income_cleaned on 'Coding'
df_merged = pd.merge(
    df_filtered,
    income_cleaned,
    on='Coding',
    how='left',
    suffixes=('', '_income')
)

#  Drop unwanted columns
low_impact_features = [
    '%SingleFamilyHome', 'Population', '%MultiFamilyHome',
    'TotalHouseholds', '%CornerHouseSingleFamily', 'LowIncomeHouseholds',
    'BelowOrAroundSocialMinimum', 'AvgGas', 'AvgHouseholdSize',
    'PctLowest40Incomes', 'GasRental', 'GasDetached', 'Age_25_45',
    'HomeWithKids', 'GasPerElecRatio', 'Gas_Apartment', 'Gas_Owned',
    'Gas_CornerHouse'
]
exclude_cols = ['Coding', 'MeanWOZValueHome'] + low_impact_features
target = 'MeanWOZValueHome'

#  Select numeric features only
X = df_merged.drop(columns=exclude_cols, errors='ignore').select_dtypes(include=[np.number])
y = pd.to_numeric(df_merged[target], errors='coerce')

#  Handle missing target values
valid_idx = y.notna()
X = X.loc[valid_idx]
y = y.loc[valid_idx]

# Impute missing values (median)
imputer = SimpleImputer(strategy='median')
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# üß™ STEP 6: Split into train/test
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, random_state=42
)

#  Train XGBoost
xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)
xgb_model.fit(X_train, y_train.to_numpy())

#  Evaluate
y_pred = xgb_model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\nüìä XGBoost Model Performance with Income Data:")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R¬≤ Score: {r2:.2f}")


# SHAP Interpretation
explainer = shap.Explainer(xgb_model, X_train)
shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test, plot_type="bar")
shap.summary_plot(shap_values, X_test)

corrs = X.corrwith(y).sort_values(ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(x=corrs.values[:15], y=corrs.index[:15])
plt.title("Top Correlated Features with WOZ Value")
plt.xlabel("Absolute Correlation with WOZ Value")  # ‚ûï X-axis label
plt.ylabel("Feature Name")                         # ‚ûï Y-axis label
plt.tight_layout()
plt.show()



# Load and preview Geo Spatial data

geospatial_df = pd.read_csv(os.path.join(data_path, 'Geo_Spatial.csv'), sep=";", index_col="ID")
geospatial_df.head()

# Rename columns in geospatial_df for clarity
geospatial_df.rename(columns={
    'WijkenEnBuurten': 'Neighborhood',
    'SoortRegio_2': 'RegionType',
    'Codering_3': 'Coding',
    'PersonenautoSPerHuishouden_109': 'CarsPerHousehold',
    'PersonenautoSNaarOppervlakte_110': 'CarsPerArea',
    'AfstandTotHuisartsenpraktijk_112': 'DistanceToGP',
    'AfstandTotGroteSupermarkt_113': 'DistanceToSupermarket',
    'AfstandTotKinderdagverblijf_114': 'DistanceToDaycare',
    'ScholenBinnen3Km_116': 'SchoolsWithin3km',
    'OppervlakteTotaal_117': 'TotalArea',
    'OppervlakteLand_118': 'LandArea',
    'OppervlakteWater_119': 'WaterArea',
    'MeestVoorkomendePostcode_120': 'CommonPostcode',
    'Dekkingspercentage_121': 'CoveragePercent',
    'MateVanStedelijkheid_122': 'UrbanityLevel',
    'Omgevingsadressendichtheid_123': 'AddressDensity'
}, inplace=True)

geospatial_df.columns

geospatial_df.info()

# Copy to avoid modifying original
geo_cleaned = geospatial_df.copy()

# Replace "." placeholders with real NaNs
geo_cleaned = geo_cleaned.replace(".", np.nan)

# Drop GM0855 row (Citywide data)
geo_cleaned = geo_cleaned[geo_cleaned['Coding'] != 'GM0855'].copy()

# Convert columns to numeric where needed
numeric_cols = [
    'CarsPerHousehold', 'CarsPerArea', 'DistanceToGP', 'DistanceToSupermarket',
    'DistanceToDaycare', 'SchoolsWithin3km', 'CommonPostcode',
    'CoveragePercent', 'UrbanityLevel', 'AddressDensity'
]
for col in numeric_cols:
    geo_cleaned[col] = pd.to_numeric(geo_cleaned[col], errors='coerce')

# Check cleaned data
print("\n Cleaned GeoSpatial Data:")
display(geo_cleaned.head())
display(geo_cleaned.dtypes)

# Remove Tilburg-wide row
geo_cleaned = geo_cleaned[geo_cleaned['Coding'] != 'GM0855'].copy()

# Drop unnecessary columns
geo_cleaned = geo_cleaned.drop(columns=['RegionType', 'CommonPostcode'])

# Replace '.' with real NaN if any remained (precaution)
geo_cleaned.replace('.', np.nan, inplace=True)

# Convert all necessary columns to numeric
for col in geo_cleaned.columns:
    if col not in ['Neighborhood', 'Coding']:  # Only exclude identifiers
        geo_cleaned[col] = pd.to_numeric(geo_cleaned[col], errors='coerce')

# Check missing values
print("\n Missing values per column BEFORE imputation:")
display(geo_cleaned.isna().sum())



imputer = SimpleImputer(strategy='median')
geo_imputed = pd.DataFrame(
    imputer.fit_transform(geo_cleaned.select_dtypes(include=[np.number])),
    columns=geo_cleaned.select_dtypes(include=[np.number]).columns
)

# Reattach Coding (non-numeric columns)
geo_imputed['Coding'] = geo_cleaned['Coding'].values

# Set index (optional)
geo_imputed.set_index(geo_cleaned.index, inplace=True)

# Final Cleaned GeoSpatial Data
print("\n Cleaned and Imputed GeoSpatial Data ready for merge:")
display(geo_imputed.head())

# Drop GM0855 from geospatial features
geo_final = geo_imputed[geo_imputed['Coding'] != 'GM0855'].copy()

#  Merge geospatial data
full_df = pd.merge(
    df_merged,
    geo_final,
    on='Coding',
    how='left'
)

# Check the result
print(f"‚úÖ Shape after merging GeoSpatial features: {full_df.shape}")
display(full_df.head())

full_df.columns



#  List of columns to permanently drop
columns_to_drop = [
    '%SingleFamilyHome', 'Population', '%MultiFamilyHome', 'TotalHouseholds',
    '%CornerHouseSingleFamily', 'LowIncomeHouseholds', 'BelowOrAroundSocialMinimum',
    'AvgGas', 'AvgHouseholdSize', 'PctLowest40Incomes', 'GasRental',
    'GasDetached', 'Age_25_45', 'HomeWithKids', 'GasPerElecRatio',
    'Gas_Apartment', 'Gas_Owned', 'Gas_CornerHouse',
    'CommonPostcode', 'CommonPostcode_x', 'CommonPostcode_income',
    'Neighborhood', 'Neighborhood_x', 'Neighborhood_y', 'RegionType',
    'Coverage%', 'CoveragePercent', 'HousingStock'
]

# üöÄ Permanently drop the columns
full_df.drop(columns=columns_to_drop, axis=1, inplace=True, errors='ignore')

print(f" Columns dropped! New shape: {full_df.shape}")

full_df.info()

# Convert object columns to numeric
object_cols = [
    '%SemiDetached_SingleFamilyHome', '%TwoUnderOneRoofHouseSingleFamily',
    '%DetachedHouseSingleFamilyHome', 'UninhabitedHome', 'OwnerOccupiedHome',
    'RentalHouseTotal', 'OwnedByHouseCorporation', 'OtherOwners',
    'BuiltLastTenYears', 'HomeWithoutKids'
]

for col in object_cols:
    full_df[col] = pd.to_numeric(full_df[col], errors='coerce')

# Handle missing values (median imputation)

# Select only numeric columns
X = full_df.drop(columns=['Coding', 'MeanWOZValueHome'])  # Drop non-features
y = full_df['MeanWOZValueHome']

# Imputer for features
imputer = SimpleImputer(strategy='median')
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# Train/test split

X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, random_state=42
)

# Train XGBoost

xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)

xgb_model.fit(X_train, y_train)

# Evaluate
y_pred = xgb_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\n Final Cleaned XGBoost Model Performance:")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R¬≤ Score: {r2:.2f}")

#SHAP Explanation

explainer = shap.Explainer(xgb_model, X_train)
shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test, plot_type="bar")
shap.summary_plot(shap_values, X_test)

# Only numerical features
corr_matrix = full_df.select_dtypes(include=[np.number]).corr()

# Plot
plt.figure(figsize=(14, 10))
sns.heatmap(corr_matrix, cmap='coolwarm', center=0, annot=False)
plt.title("üîç Correlation Matrix of Features", fontsize=16)
plt.show()



"""Adding the GeoSpatial features didn't improve my model performance, removing most of it as I did below didn't improve it either as I already had similar performance before. I will add the distance to fire staion data to see how it affects my model performance

"""

# üî• Step 1: Define columns to DROP
geo_columns_to_drop = [
    'DistanceToGP', 'DistanceToSupermarket', 'DistanceToDaycare', 'SchoolsWithin3km',
    'TotalArea', 'LandArea', 'WaterArea', 'CoveragePercent'
]

# üî• Step 2: Drop these columns permanently
full_df.drop(columns=geo_columns_to_drop, inplace=True, errors='ignore')

print(f"‚úÖ Shape after dropping unnecessary geospatial columns: {full_df.shape}")

# üî• Step 3: Continue with your usual model building
# (same as before ‚Äî define X, y, impute missing values, train XGBoost, evaluate)

# Example:

# Define features and target
target = 'MeanWOZValueHome'
exclude_cols = ['Coding', target]
X = full_df.drop(columns=exclude_cols, errors='ignore').select_dtypes(include=[np.number])
y = pd.to_numeric(full_df[target], errors='coerce')

# Drop missing target rows
valid_idx = y.notna()
X = X.loc[valid_idx]
y = y.loc[valid_idx]

# Impute missing features
imputer = SimpleImputer(strategy='median')
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, random_state=42
)

# Train XGBoost
xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)
xgb_model.fit(X_train, y_train.to_numpy())

# Predict
y_pred = xgb_model.predict(X_test)

# Evaluate
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # fixed the squared=False mistake
r2 = r2_score(y_test, y_pred)

print("\nüìä New Cleaned XGBoost Model Performance:")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R¬≤ Score: {r2:.2f}")

# SHAP
explainer = shap.Explainer(xgb_model, X_train)
shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test, plot_type="bar")
shap.summary_plot(shap_values, X_test)





#  Load the file containing distance to fire station
fire_df = pd.read_csv(
    os.path.join(data_path, 'proximity85830NED_UntypedDataSet_22032025_101506.csv'),
    sep=";", index_col="ID"
)

fire_df.tail()

fire_df.info()

# 1. Select only the useful columns
fire_features = fire_df[[
    'Codering_3',
    'AfstandTotBrandweerkazerne_114',
    'AfstandTotTreinstationsTotaal_90',
    'AfstandTotBelangrijkOverstapstation_91'
]].copy()

# 2. Rename columns
fire_features.rename(columns={
    'Codering_3': 'Coding',
    'AfstandTotBrandweerkazerne_114': 'DistanceToFireStation',
    'AfstandTotTreinstationsTotaal_90': 'DistanceToTrainStation',
    'AfstandTotBelangrijkOverstapstation_91': 'DistanceToMajorTransferStation'
}, inplace=True)

# 3. Replace '.' with NaN and convert columns to numeric
for col in fire_features.columns[1:]:
    fire_features[col] = fire_features[col].replace('.', np.nan)
    fire_features[col] = pd.to_numeric(fire_features[col], errors='coerce')

# 4. Drop any rows where 'Coding' is missing (if any)
fire_features = fire_features.dropna(subset=['Coding'])

# 5. Final Check
print(f"‚úÖ Fire features final shape (after excluding bad columns): {fire_features.shape}")
display(fire_features.head())



"""As seen below, adding distance to fire service station didn't improve model performance either."""

# üõ†Ô∏è Merge fire station features into full_df
# (Remember: drop GM0855 ('Coding' == 'GM0855') if needed)

# 1. Merge on 'Coding'
final_df = pd.merge(
    full_df, fire_features,
    on='Coding',
    how='left'   # left join to preserve your modeling rows
)

# 2. Define target and features
target = 'MeanWOZValueHome'
X = final_df.drop(columns=[target, 'Coding'], errors='ignore')
y = final_df[target]

# 3. Convert object types to numeric where necessary
X = X.apply(pd.to_numeric, errors='coerce')

# 4. Impute missing values with median
imputer = SimpleImputer(strategy='median')
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# 5. Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, random_state=42
)

# 6. Train XGBoost model
xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)
xgb_model.fit(X_train, y_train)

# 7. Predict and evaluate
y_pred = xgb_model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\nüìä Final Model Performance (with Fire Station Proximity):")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R¬≤ Score: {r2:.2f}")

# 8. SHAP Interpretation
explainer = shap.Explainer(xgb_model, X_train)
shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test, plot_type="bar")
shap.summary_plot(shap_values, X_test)





"""I tried a model with variables strictly describing houses or it's loction. Not those describing people that live in it to compare performance below.

"""

selected_features = [
    '%SemiDetached_SingleFamilyHome',
    '%DetachedHouseSingleFamilyHome',
    'BuiltLastTenYears',
    'PopulationDensity',
    #'DistanceToGP',
    #'DistanceToSupermarket',
    'DistanceToTrainStation',
    'DistanceToFireStation',   # üÜï
    'UrbanityLevel',
    'AddressDensity'
]



final_df.columns

# Add DistanceToFireStation and build the model

X_small = final_df[selected_features].copy()
y_small = final_df['MeanWOZValueHome'].copy()

# Impute missing values
imputer = SimpleImputer(strategy='median')
X_small_imputed = pd.DataFrame(imputer.fit_transform(X_small), columns=X_small.columns)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X_small_imputed, y_small, test_size=0.2, random_state=42)

# Train simple XGBoost
xgb_small = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)
xgb_small.fit(X_train, y_train.to_numpy())

# Predict and evaluate
y_pred = xgb_small.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\nüìä Physical Feature XGBoost Model Performance (with DistanceToFireStation):")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R¬≤ Score: {r2:.2f}")

# Shap analysis of physical factors on the prediction
explainer_simple = shap.Explainer(xgb_small, X_train)
shap_values_simple = explainer_simple(X_test)

shap.summary_plot(shap_values_simple, X_test, plot_type="bar")
shap.summary_plot(shap_values_simple, X_test)



# Random forest on physical features
# Random Forest Regressor
rf_model_small = RandomForestRegressor(n_estimators=200, random_state=42)
rf_model_small.fit(X_train, y_train)

# Predict
y_pred_rf = rf_model_small.predict(X_test)

# Metrics
mae_rf = mean_absolute_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)

print("\nüå≤ Random Forest Performance (Physical Features Only):")
print(f"MAE: {mae_rf:.2f}")
print(f"RMSE: {rmse_rf:.2f}")
print(f"R¬≤ Score: {r2_rf:.2f}")



# Base models for stacking
base_models = [
    ('rf', RandomForestRegressor(n_estimators=200, random_state=42)),
    ('ridge', Ridge(alpha=1.0))
]

# Final model
final_model = Ridge(alpha=1.0)

# Stacking Regressor
stacked_model = StackingRegressor(
    estimators=base_models,
    final_estimator=final_model,
    passthrough=True,
    n_jobs=-1
)

# Train
stacked_model.fit(X_train, y_train)

# Predict
y_pred_stack = stacked_model.predict(X_test)

# Metrics
mae_stack = mean_absolute_error(y_test, y_pred_stack)
rmse_stack = np.sqrt(mean_squared_error(y_test, y_pred_stack))
r2_stack = r2_score(y_test, y_pred_stack)

print("\nüß© Stacked Model (RandomForest + Ridge) Performance (Physical Features Only):")
print(f"MAE: {mae_stack:.2f}")
print(f"RMSE: {rmse_stack:.2f}")
print(f"R¬≤ Score: {r2_stack:.2f}")



# Only use the RandomForest inside the stacker for SHAP
rf_model = stacked_model.named_estimators_['rf']

# Create SHAP explainer
explainer = shap.Explainer(rf_model, X_test)
shap_values = explainer(X_test)

# Bar plot (mean SHAP value)
shap.summary_plot(shap_values, X_test, plot_type="bar")

# Beeswarm (detailed plot)
shap.summary_plot(shap_values, X_test)

#  your correlation matrix

X_small_imputed_with_target = X_small_imputed.copy()
X_small_imputed_with_target['MeanWOZValueHome'] = y_small.values

# Plot correlation including target
plt.figure(figsize=(12, 10))
corr_matrix = X_small_imputed_with_target.corr()

sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, fmt=".2f", square=True, cbar_kws={"shrink": .8})
plt.title("üîµ Correlation Matrix (Physical Features + Target: MeanWOZValueHome)")
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()



# Predict again if needed
y_pred_rf = rf_model.predict(X_test)
y_pred_stacked = stacked_model.predict(X_test)

# Create a DataFrame for comparing true vs predicted values

comparison_df = pd.DataFrame({
    'Actual': y_test,
    'RandomForest_Predicted': y_pred_rf,
    'StackedModel_Predicted': y_pred_stacked
})

# 2. Heatmap of Errors
plt.figure(figsize=(8, 6))
error_corr = comparison_df.corr()

sns.heatmap(error_corr, cmap='coolwarm', annot=True, fmt=".2f", square=True, cbar_kws={"shrink": .8})
plt.title("üîµ Correlation of Actual vs Model Predictions")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()



# Final model results
model_results = {
    'Model': [
        'XGBoost (Full)',
        'Voting (RF + XGB)',
        'XGBoost (Physical Only)',
        'Random Forest (Physical Only)',
        'Stacked Model (Physical Only)',
        'LightGBM (Full)',
        'Random Forest (Baseline)'
    ],
    'MAE': [21.94, 21.94, 31.90, 45.69, 43.13, 27.21, 33.40],
    'RMSE': [31.99, 30.85, 63.69, 65.83, 58.91, 38.49, 52.82],
    'R2': [0.94, 0.95, 0.77, 0.75, 0.79, 0.80, 0.91]
}

df_results = pd.DataFrame(model_results)

# Plotting
fig, axes = plt.subplots(3, 1, figsize=(12, 18))

# MAE Plot
axes[0].barh(df_results['Model'], df_results['MAE'], color='skyblue')
axes[0].set_title('MAE Comparison Across Models')
axes[0].invert_yaxis()
axes[0].set_xlabel('Mean Absolute Error (lower is better)')

# RMSE Plot
axes[1].barh(df_results['Model'], df_results['RMSE'], color='lightgreen')
axes[1].set_title('RMSE Comparison Across Models')
axes[1].invert_yaxis()
axes[1].set_xlabel('Root Mean Squared Error (lower is better)')

# R2 Score Plot
axes[2].barh(df_results['Model'], df_results['R2'], color='salmon')
axes[2].set_title('R¬≤ Score Comparison Across Models')
axes[2].invert_yaxis()
axes[2].set_xlabel('R¬≤ Score (higher is better)')

plt.tight_layout()
plt.show()









# ‚úÖ Merge fire_features to df_merged
df_fire_merged = df_merged.merge(fire_features, on='Coding', how='left')

# ‚úÖ Drop the low-impact features
low_impact_features = [
    '%SingleFamilyHome', 'Population', '%MultiFamilyHome',
    'TotalHouseholds', '%CornerHouseSingleFamily', 'LowIncomeHouseholds',
    'BelowOrAroundSocialMinimum', 'AvgGas', 'AvgHouseholdSize',
    'PctLowest40Incomes', 'GasRental', 'GasDetached', 'Age_25_45',
    'HomeWithKids', 'GasPerElecRatio', 'Gas_Apartment', 'Gas_Owned',
    'Gas_CornerHouse'
]

target = 'MeanWOZValueHome'
exclude_cols = ['Coding', target] + low_impact_features

# üì¶ Select numeric features
X = df_fire_merged.drop(columns=exclude_cols, errors='ignore').select_dtypes(include=[np.number])
y = pd.to_numeric(df_fire_merged[target], errors='coerce')

# üõ†Ô∏è Drop rows with missing target
valid_idx = y.notna()
X = X.loc[valid_idx]
y = y.loc[valid_idx]

# üßΩ Impute missing values
imputer = SimpleImputer(strategy='median')
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# üîÄ Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, random_state=42
)

# üöÄ Train model
xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)
xgb_model.fit(X_train, y_train.to_numpy())

# üìà Evaluate
y_pred = xgb_model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\nüìä Model Performance (Fire Station added to df_merged, no GeoSpatial):")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R¬≤ Score: {r2:.2f}")

# üìä SHAP analysis (optional)
explainer = shap.Explainer(xgb_model, X_train)
shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test, plot_type="bar")
shap.summary_plot(shap_values, X_test)

"""Let's try other algorithms besides just XGBoost that have been used so far. Maybe even an emsembled method

"""

# Train RandomForest
rf_model = RandomForestRegressor(
    n_estimators=200,
    max_depth=8,
    random_state=42
)
rf_model.fit(X_train, y_train)

# Predict
rf_pred = rf_model.predict(X_test)

# Evaluate
rf_mae = mean_absolute_error(y_test, rf_pred)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))
rf_r2 = r2_score(y_test, rf_pred)

print("\nüå≤ Random Forest Performance:")
print(f"MAE: {rf_mae:.2f}")
print(f"RMSE: {rf_rmse:.2f}")
print(f"R¬≤ Score: {rf_r2:.2f}")

# Train LightGBM
lgb_model = LGBMRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=8,
    random_state=42
)
lgb_model.fit(X_train, y_train)

# Predict
lgb_pred = lgb_model.predict(X_test)

# Evaluate
lgb_mae = mean_absolute_error(y_test, lgb_pred)
lgb_rmse = np.sqrt(mean_squared_error(y_test, lgb_pred))
lgb_r2 = r2_score(y_test, lgb_pred)

print("\n‚ö° LightGBM Performance:")
print(f"MAE: {lgb_mae:.2f}")
print(f"RMSE: {lgb_rmse:.2f}")
print(f"R¬≤ Score: {lgb_r2:.2f}")

# Define the base models
rf_model = RandomForestRegressor(
    n_estimators=200,
    max_depth=6,
    random_state=42
)

xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)

# Fit the models individually first
rf_model.fit(X_train, y_train)
xgb_model.fit(X_train, y_train)

# Create a Voting Regressor
voting_model = VotingRegressor(
    estimators=[
        ('rf', rf_model),
        ('xgb', xgb_model)
    ]
)

# Fit the Voting Regressor
voting_model.fit(X_train, y_train)

# Predict and Evaluate
y_pred = voting_model.predict(X_test)

# Metrics
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\nüéØ Voting Regressor (RandomForest + XGBoost) Performance:")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R¬≤ Score: {r2:.2f}")





"""Test model scalability with data from Breda

"""

breda_df = pd.read_csv(os.path.join(data_path, 'Breda_data.csv'), sep=";", index_col="ID")
breda_df.head()

breda_df.info()

breda_df.shape

breda_df_2 = pd.read_csv(os.path.join(data_path, 'Breda_data_2.csv'), sep=";", index_col="ID")
breda_df_2.head()

breda_df_2.info()

breda_df_2.head()

# Exclude "GM0758" (the general Breda municipality row)
breda_df = breda_df[~breda_df['Codering_3'].str.startswith('GM')]
breda_df_2 = breda_df_2[~breda_df_2['Codering_3'].str.startswith('GM')]

# Select useful columns from breda_df
breda_main = breda_df[[
    'Codering_3',                     # For merging
    'GemiddeldeWOZWaardeVanWoningen_36', # Mean WOZ Value
    'PercentageEengezinswoning_37',
    'TweeOnderEenKapWoning_52',
    'VrijstaandeWoning_53',
    'BouwjaarVanaf2000_47',
    'Omgevingsadressendichtheid_123',  # Population Density
    'MateVanStedelijkheid_122',        # Urbanity Level
    'AfstandTotGroteSupermarkt_113'    # Distance to Supermarket
]].copy()

# Select only useful and clean columns from breda_df_2
# Check for columns that don't have weird "." entries
columns_to_keep = [
    'Codering_3',
    'AfstandTotHuisartsenpraktijk_5',  # Distance to GP
    'AfstandTotTreinstationsTotaal_90', # Distance to Train Station
    'AfstandTotBrandweerkazerne_114'    # Distance to Fire Station
]
breda_proximity = breda_df_2[columns_to_keep].copy()

# Rename columns to match Tilburg format
breda_main.rename(columns={
    'Codering_3': 'Coding',
    'GemiddeldeWOZWaardeVanWoningen_36': 'MeanWOZValueHome',
    'PercentageEengezinswoning_37': '%SingleFamilyHome',
    'TweeOnderEenKapWoning_52': '%SemiDetached_SingleFamilyHome',
    'VrijstaandeWoning_53': '%DetachedHouseSingleFamilyHome',
    'BouwjaarVanaf2000_47': 'BuiltLastTenYears',
    'Omgevingsadressendichtheid_123': 'PopulationDensity',
    'MateVanStedelijkheid_122': 'UrbanityLevel',
    'AfstandTotGroteSupermarkt_113': 'DistanceToSupermarket'
}, inplace=True)

breda_proximity.rename(columns={
    'Codering_3': 'Coding',
    'AfstandTotHuisartsenpraktijk_5': 'DistanceToGP',
    'AfstandTotTreinstationsTotaal_90': 'DistanceToTrainStation',
    'AfstandTotBrandweerkazerne_114': 'DistanceToFireStation'
}, inplace=True)

# Merge the two cleaned datasets
merged_breda = pd.merge(breda_main, breda_proximity, on='Coding', how='inner')

# Convert all columns except 'Coding' to numeric
for col in merged_breda.columns:
    if col != 'Coding':
        merged_breda[col] = pd.to_numeric(merged_breda[col], errors='coerce')

# Handle missing values if necessary (for example, using median imputation later)
# merged_breda = merged_breda.fillna(merged_breda.median())

#  Check final dataset
print("‚úÖ Final Breda Cleaned Shape:", merged_breda.shape)
display(merged_breda.head())

merged_breda.info()



"""Retrain XGBoost again on features describing houses which also appear in the Breda data

"""

selected_features = [
    '%SemiDetached_SingleFamilyHome',
    '%DetachedHouseSingleFamilyHome',
    'BuiltLastTenYears',
    'PopulationDensity',
    'DistanceToTrainStation',
    'DistanceToFireStation',
    'UrbanityLevel'
]

# X = final_df[selected_features]
X_small = final_df[selected_features].copy()
y_small = final_df['MeanWOZValueHome'].copy()

# Impute missing values if needed
imputer = SimpleImputer(strategy='median')
X_small_imputed = pd.DataFrame(imputer.fit_transform(X_small), columns=X_small.columns)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X_small_imputed, y_small, test_size=0.2, random_state=42)

# Train XGBoost
xgb_small = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)
xgb_small.fit(X_train, y_train)

# Predict on Tilburg test
y_pred = xgb_small.predict(X_test)
print(f"Train/Test MAE: {mean_absolute_error(y_test, y_pred):.2f}")
print(f"Train/Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")
print(f"Train/Test R¬≤: {r2_score(y_test, y_pred):.2f}")

"""Then predict on Breda below"""

# Remove rows where Breda actual WOZ is missing
breda_valid = merged_breda.dropna(subset=['MeanWOZValueHome'])

X_breda_valid = breda_valid[selected_features]
y_breda_actual = breda_valid['MeanWOZValueHome']

# Impute X (features)
X_breda_imputed = pd.DataFrame(imputer.transform(X_breda_valid), columns=X_breda_valid.columns)

# Predict
y_breda_pred = xgb_small.predict(X_breda_imputed)

# Now evaluate!
print("\nüìä Breda Test Results:")
print(f"MAE: {mean_absolute_error(y_breda_actual, y_breda_pred):.2f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_breda_actual, y_breda_pred)):.2f}")
print(f"R¬≤: {r2_score(y_breda_actual, y_breda_pred):.2f}")

print(breda_valid[selected_features].describe())

print(final_df[selected_features].describe())



"""The performance of the model on Breda data is not proper and the key observations are:

1. %SemiDetached and %Detached look totally wrong in Breda	‚ùó Values like 3368, 4275 are not percentages but seem like absolute electricity consumption or another metric.
2. Population Density is very different	5835 (Tilburg) vs 1877 (Breda) ‚Üí ‚ö†Ô∏è Model trained on much denser area.
3. Other distance features (fire station, train) look okayish!

So, Retrain model on Tilburg without using %SemiDetached and %DetachedHouse (to simulate fair comparison).

Test again on Breda (only use "healthy" features).


"""

# Selected features
selected_clean_features = [
    'BuiltLastTenYears',
    'PopulationDensity',
    'DistanceToTrainStation',
    'DistanceToFireStation',
    'UrbanityLevel'
]

# Prepare train (Tilburg)
X_small = final_df[selected_clean_features].copy()
y_small = final_df['MeanWOZValueHome'].copy()

# Impute missing values for Tilburg
imputer = SimpleImputer(strategy='median')
X_small_imputed = pd.DataFrame(imputer.fit_transform(X_small), columns=X_small.columns)

# Train/test split on Tilburg
X_train, X_test, y_train, y_test = train_test_split(X_small_imputed, y_small, test_size=0.2, random_state=42)

# Retrain model
xgb_small_clean = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)
xgb_small_clean.fit(X_train, y_train)

# Evaluate on Tilburg
y_pred_train = xgb_small_clean.predict(X_test)

print("\nüìä Cleaned Tilburg Model Performance (internal):")
print(f"MAE: {mean_absolute_error(y_test, y_pred_train):.2f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_train)):.2f}")
print(f"R¬≤ Score: {r2_score(y_test, y_pred_train):.2f}")

# Prepare test (Breda)

# Drop Breda rows where MeanWOZValueHome is missing
merged_breda_clean = merged_breda.dropna(subset=['MeanWOZValueHome']).copy()

# Drop Breda rows where feature values are missing
merged_breda_clean = merged_breda_clean.dropna(subset=selected_clean_features).copy()

print(f"\n‚úÖ Breda cleaned shape for testing: {merged_breda_clean.shape}")

# Prepare Breda input
X_breda = merged_breda_clean[selected_clean_features].copy()
y_breda_actual = merged_breda_clean['MeanWOZValueHome'].copy()

# Impute Breda (use the same imputer trained on Tilburg)
X_breda_imputed = pd.DataFrame(imputer.transform(X_breda), columns=X_breda.columns)

# Predict on Breda
y_breda_pred = xgb_small_clean.predict(X_breda_imputed)

print("\nüìä Breda Test Results (Cleaned Model):")
print(f"MAE: {mean_absolute_error(y_breda_actual, y_breda_pred):.2f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_breda_actual, y_breda_pred)):.2f}")
print(f"R¬≤ Score: {r2_score(y_breda_actual, y_breda_pred):.2f}")

# Scatter plot: Actual vs Predicted
plt.figure(figsize=(8, 6))
plt.scatter(y_breda_actual, y_breda_pred, alpha=0.7)
plt.plot([y_breda_actual.min(), y_breda_actual.max()],
         [y_breda_actual.min(), y_breda_actual.max()],
         'r--', lw=2)
plt.xlabel('Actual WOZ Value (Breda)')
plt.ylabel('Predicted WOZ Value (Breda)')
plt.title('üìà Actual vs Predicted - Breda Test Set')
plt.grid(True)
plt.show()

# Residual plot: Errors
residuals = y_breda_actual - y_breda_pred

plt.figure(figsize=(8, 6))
plt.scatter(y_breda_actual, residuals, alpha=0.7)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('Actual WOZ Value (Breda)')
plt.ylabel('Residuals (Actual - Predicted)')
plt.title('üìâ Residual Plot - Breda Test Set')
plt.grid(True)
plt.show()





"""Clean data from Delft, Den Bosch, Dordrecht, Eindhoven and Nijmegen and text my model on it for scalability"""

DDDEN_proximity = pd.read_csv(os.path.join(data_path, 'proximity_DDDEN.csv'), sep=";", index_col="ID")
DDDEN_df = pd.read_csv(os.path.join(data_path, 'DDDEN.csv'), sep=";", index_col="ID")

DDDEN_df.info()

DDDEN_df.head()

DDDEN_proximity.info()

DDDEN_proximity.shape

DDDEN_proximity.head()

# Columns to select and rename
dden_selected = DDDEN_df[[
    'Codering_3',
    'GemiddeldeWOZWaardeVanWoningen_39',
    'PercentageTweeOnderEenKapWoningEe_43',
    'PercentageVrijstaandeWoningEengezins_44',
    'BouwjaarAfgelopenTienJaar_52',
    'Omgevingsadressendichtheid_128',
    'MateVanStedelijkheid_127'
]].copy()

# Rename columns
dden_selected.rename(columns={
    'Codering_3': 'Coding',
    'GemiddeldeWOZWaardeVanWoningen_39': 'MeanWOZValueHome',
    'PercentageTweeOnderEenKapWoningEe_43': '%SemiDetached_SingleFamilyHome',
    'PercentageVrijstaandeWoningEengezins_44': '%DetachedHouseSingleFamilyHome',
    'BouwjaarAfgelopenTienJaar_52': 'BuiltLastTenYears',
    'Omgevingsadressendichtheid_128': 'PopulationDensity',
    'MateVanStedelijkheid_127': 'UrbanityLevel'
}, inplace=True)

# Replace '.' with NaN
dden_selected.replace('.', np.nan, inplace=True)

# Convert all columns except Coding to numeric
for col in dden_selected.columns:
    if col != 'Coding':
        dden_selected[col] = pd.to_numeric(dden_selected[col], errors='coerce')

# Drop rows where MeanWOZValueHome is missing (can't test without target)
dden_selected.dropna(subset=['MeanWOZValueHome'], inplace=True)

# Check the result
print("\n‚úÖ Cleaned DDEN Shape:", dden_selected.shape)
display(dden_selected.head())

dden_selected.info()

# Clean DDDEN_proximity
ddden_proximity_clean = DDDEN_proximity[[
    'Codering_3',
    'AfstandTotTreinstationsTotaal_90',
    'AfstandTotBrandweerkazerne_114'
]].copy()

# Rename columns nicely
ddden_proximity_clean.rename(columns={
    'Codering_3': 'Coding',
    'AfstandTotTreinstationsTotaal_90': 'DistanceToTrainStation',
    'AfstandTotBrandweerkazerne_114': 'DistanceToFireStation'
}, inplace=True)

# Replace '.' with NaN
ddden_proximity_clean.replace('.', np.nan, inplace=True)

# Convert Distance columns to numeric
ddden_proximity_clean['DistanceToTrainStation'] = pd.to_numeric(ddden_proximity_clean['DistanceToTrainStation'], errors='coerce')
ddden_proximity_clean['DistanceToFireStation'] = pd.to_numeric(ddden_proximity_clean['DistanceToFireStation'], errors='coerce')

# Drop rows where important distances are missing
ddden_proximity_clean.dropna(subset=['DistanceToTrainStation', 'DistanceToFireStation'], inplace=True)

print("‚úÖ Cleaned DDDEN Proximity Shape:", ddden_proximity_clean.shape)
ddden_proximity_clean.head()

# Merge on 'Coding'
merged_ddden = pd.merge(
    dden_selected,
    ddden_proximity_clean,
    on='Coding',
    how='inner'   # Only rows that exist in both
)

print("‚úÖ Merged DDDEN Shape:", merged_ddden.shape)
merged_ddden.head()

# Drop rows where Coding starts with 'GM'
merged_ddden = merged_ddden[~merged_ddden['Coding'].str.startswith('GM')]

print("‚úÖ Final Cleaned DDDEN Shape:", merged_ddden.shape)
merged_ddden.head()



selected_features = [
    '%SemiDetached_SingleFamilyHome',
    '%DetachedHouseSingleFamilyHome',
    'BuiltLastTenYears',
    'PopulationDensity',
    #'DistanceToGP',
    #'DistanceToSupermarket',
    'DistanceToTrainStation',
    'DistanceToFireStation',   # üÜï
    'UrbanityLevel',
    'AddressDensity'
]

# Select the features you used in the Tilburg physical model
selected_clean_features = [
    'BuiltLastTenYears',
    'PopulationDensity',
    'DistanceToTrainStation',
    'DistanceToFireStation',
    'UrbanityLevel'
]

# Prepare Breda X and y
X_ddden = merged_ddden[selected_clean_features].copy()
y_ddden_actual = merged_ddden['MeanWOZValueHome'].copy()

# If needed, impute missing (e.g. median imputer you trained before)
X_ddden_imputed = pd.DataFrame(imputer.transform(X_ddden), columns=X_ddden.columns)

# Predict
y_ddden_pred = xgb_small_clean.predict(X_ddden_imputed)

# Evaluate
print("\nüìä DDDEN Test Results:")
print(f"MAE: {mean_absolute_error(y_ddden_actual, y_ddden_pred):.2f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_ddden_actual, y_ddden_pred)):.2f}")
print(f"R¬≤ Score: {r2_score(y_ddden_actual, y_ddden_pred):.2f}")





"""Train and test an xgboost on the ddden dataset"""

# üìã Define the selected features
selected_features_ddden = [
    '%SemiDetached_SingleFamilyHome',
    '%DetachedHouseSingleFamilyHome',
    'BuiltLastTenYears',
    'PopulationDensity',
    'UrbanityLevel',
    'DistanceToTrainStation',
    'DistanceToFireStation'
]

# üßπ Prepare features and target
X_ddden = merged_ddden[selected_features_ddden].copy()
y_ddden = merged_ddden['MeanWOZValueHome'].copy()

# üöø Handle missing values (if any)
imputer_ddden = SimpleImputer(strategy='median')
X_ddden_imputed = pd.DataFrame(imputer_ddden.fit_transform(X_ddden), columns=X_ddden.columns)

# ‚úÇÔ∏è Train/Test split (if you want internal validation)
X_train_ddden, X_test_ddden, y_train_ddden, y_test_ddden = train_test_split(
    X_ddden_imputed, y_ddden, test_size=0.2, random_state=42
)

# ‚ö° Train XGBoost Model
xgb_ddden = xgb.XGBRegressor(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)

xgb_ddden.fit(X_train_ddden, y_train_ddden)

# üéØ Predict on test set
y_pred_ddden = xgb_ddden.predict(X_test_ddden)

# üßÆ Evaluate model
mae = mean_absolute_error(y_test_ddden, y_pred_ddden)
rmse = np.sqrt(mean_squared_error(y_test_ddden, y_pred_ddden))
r2 = r2_score(y_test_ddden, y_pred_ddden)

print("\nüìä XGBoost Model on DDDEN Data:")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R¬≤ Score: {r2:.2f}")

import matplotlib.pyplot as plt

model_names = ['Baseline', 'Full Features', 'Physical Features', 'Breda', 'DDDEN']
rmse_scores = [52.82, 31.99, 58.91, 72.18, 100.08]
r2_scores = [0.84, 0.94, 0.80, 0.70, 0.14]

x = range(len(model_names))

plt.figure(figsize=(10, 5))
plt.plot(x, rmse_scores, marker='o', label='RMSE')
plt.plot(x, r2_scores, marker='o', label='R¬≤', color='green')
plt.xticks(x, model_names, rotation=45)
plt.ylabel('Score')
plt.title('Model Performance: RMSE and R¬≤')
plt.legend()
plt.tight_layout()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

model_names = ['Baseline', 'Full Features', 'Physical Features', 'Breda', 'DDDEN']
rmse_scores = [52.82, 31.99, 58.91, 72.18, 100.08]
r2_scores = [0.84, 0.94, 0.80, 0.70, 0.14]
mae_scores = [33.40, 21.94, 43.13, 52.33, 68.37]

x = range(len(model_names))

plt.figure(figsize=(12, 6))

# Plot all three metrics
plt.plot(x, rmse_scores, marker='o', label='RMSE', color='blue')
plt.plot(x, r2_scores, marker='o', label='R¬≤', color='green')
plt.plot(x, mae_scores, marker='o', label='MAE', color='red')

plt.xticks(x, model_names, rotation=45)
plt.ylabel('Score')
plt.title('Model Performance Comparison')
plt.legend()
plt.tight_layout()
plt.grid(True, linestyle='--', alpha=0.7)

# Add value labels above each point
for i, (rmse, r2, mae) in enumerate(zip(rmse_scores, r2_scores, mae_scores)):
    plt.text(i, rmse+3, f'{rmse:.1f}', ha='center', color='blue')
    plt.text(i, r2+0.03, f'{r2:.2f}', ha='center', color='green')
    plt.text(i, mae+3, f'{mae:.1f}', ha='center', color='red')

plt.show()

# Bar plot version of Figure 15
metrics = ['MAE', 'RMSE', 'R¬≤']
models = ['Baseline', 'Full Features', 'Physical Only']
values = [[33.40, 21.94, 43.13],
          [52.82, 31.99, 58.91],
          [0.84, 0.94, 0.80]]

fig, axes = plt.subplots(1, 3, figsize=(18, 6))

for i, (metric, vals) in enumerate(zip(metrics, values)):
    bars = axes[i].bar(models, vals, color=['#1f77b4', '#ff7f0e', '#2ca02c'])
    axes[i].set_title(metric, fontsize=14)
    axes[i].bar_label(bars, fmt='%.2f', padding=3)

plt.suptitle('Model Performance Comparison Across Metrics', y=1.05, fontsize=24)
plt.tight_layout()
plt.savefig('performance_comparison.pdf')