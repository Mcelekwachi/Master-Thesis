# -*- coding: utf-8 -*-
"""Michael_Thesis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1onHW0lK2Ab6Yv4JO7UdxvQBEKcLO-0Zl
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import shap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import RandomForestRegressor, StackingRegressor
from sklearn.ensemble import VotingRegressor
from sklearn.linear_model import Ridge
from lightgbm import LGBMRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.inspection import permutation_importance

data_path = "/content/drive/MyDrive/New CBS"
os.listdir(data_path)

#read and preview first dataframe from google drive

housing_df = pd.read_csv(os.path.join(data_path, 'Housing & Real Estate.csv'), sep=";", index_col="ID")
housing_df.head()

housing_df.shape

# rename columns

new_column_names = {
    'WijkenEnBuurten': 'Neighborhoods',
    'Codering_3': 'Coding',
    'IndelingswijzigingGemeenteWijkBuurt_4': 'NeighborhoodChanges',
    'Woningvoorraad_35': 'HousingStock',
    'NieuwbouwWoningen_36': 'NewHome',
    'GemiddeldeWOZWaardeVanWoningen_39': 'MeanWOZValueHome',
    'PercentageEengezinswoning_40': '%SingleFamilyHome',
    'PercentageTussenwoningEengezins_41': '%SemiDetached_SingleFamilyHome',
    'PercentageHoekwoningEengezins_42': '%CornerHouseSingleFamily',
    'PercentageTweeOnderEenKapWoningEe_43': '%TwoUnderOneRoofHouseSingleFamily',
    'PercentageVrijstaandeWoningEengezins_44': '%DetachedHouseSingleFamilyHome',
    'PercentageMeergezinswoning_45': '%MultiFamilyHome',
    'OnbewoondeWoningen_46': 'UninhabitedHome',
    'Koopwoningen_47': 'OwnerOccupiedHome',
    'HuurwoningenTotaal_48': 'RentalHouseTotal',
    'InBezitWoningcorporatie_49': 'OwnedByHouseCorporation',
    'InBezitOverigeVerhuurders_50': 'OtherOwners',
    'BouwjaarAfgelopenTienJaar_52': 'BuiltLastTenYears',
    'MeestVoorkomendePostcode_125': 'CommonPostcode',
    'Dekkingspercentage_126': 'Coverage%'
}

housing_df = housing_df.rename(columns=new_column_names)

housing_df.head()

# Strip trailing and leading whitespaces from 'Neighborhoods' and 'Coding' columns
housing_df['Neighborhoods'] = housing_df['Neighborhoods'].str.strip()
housing_df['Coding'] = housing_df['Coding'].str.strip()

# Optional: confirm fix by displaying unique values again
print(housing_df['Neighborhoods'].unique()[:7])
print(housing_df['Coding'].unique()[:7])

housing_df.drop(columns=["NewHome", "Neighborhoods"], inplace=True)

housing_df.columns

housing_df.info()

#copy dataframe
df = housing_df.copy()

# Convert appropriate columns to correct data types
df['Coding'] = df['Coding'].astype(str)

# Columns to convert to numeric (float) — remove % symbols if they exist
numeric_cols = [
    'HousingStock', 'MeanWOZValueHome', '%SingleFamilyHome', '%SemiDetached_SingleFamilyHome',
    '%CornerHouseSingleFamily', '%TwoUnderOneRoofHouseSingleFamily',
    '%DetachedHouseSingleFamilyHome', '%MultiFamilyHome', 'UninhabitedHome',
    'OwnerOccupiedHome', 'RentalHouseTotal', 'OwnedByHouseCorporation',
    'OtherOwners', 'BuiltLastTenYears', 'Coverage%'
]

# Remove percentage signs if present and convert
for col in numeric_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce')  # convert to float, NaNs if invalid

# Optional: check conversion
print(df[numeric_cols].dtypes)
df[numeric_cols].head()

df.info()

print("\nMissing values:\n", df.isnull().sum())  # Count missing values

df.shape

df_cleaned = df.dropna(thresh=8)  # Keeps rows with at least 8 non-NA values

print("\nMissing values:\n", df_cleaned.isnull().sum())  # Count missing values

df_cleaned.head()

#  Histogram of WOZ values
plt.hist(df_cleaned['MeanWOZValueHome'], bins=20, edgecolor='black')
plt.title('Distribution of WOZ Home Values')
plt.xlabel('WOZ Value (x €100k)')
plt.ylabel('Number of Neighborhoods')
plt.xticks(rotation=30, ha='right')
plt.tight_layout()
plt.show()

# Define the housing type columns
housing_type_cols = [
    '%SingleFamilyHome',
    '%SemiDetached_SingleFamilyHome',
    '%CornerHouseSingleFamily',
    '%TwoUnderOneRoofHouseSingleFamily',
    '%DetachedHouseSingleFamilyHome',
    '%MultiFamilyHome'
]

# Calculate the average percentage for each housing type
avg_composition = df_cleaned[housing_type_cols].mean()

# Create the vertical bar plot using matplotlib
plt.figure(figsize=(10, 6))
plt.bar(avg_composition.index, avg_composition.values, color='skyblue', edgecolor='black')

# Customize plot
plt.title('Average Housing Type Composition Across Neighborhoods')
plt.ylabel('Percentage')
plt.xlabel('Housing Type')
plt.xticks(rotation=30, ha='right')
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()

# Show the plot
plt.show()





demographics_df = pd.read_csv(os.path.join(data_path, 'Demographics & Households.csv'), sep=";", index_col="ID")
demographics_df.head()

demographics_df.info()

demographics_df.columns

# Copy to avoid changing the original
df = demographics_df.copy()

# Rename columns for clarity
df.rename(columns={
    'WijkenEnBuurten': 'Neighborhood',
    'SoortRegio_2': 'RegionType',
    'Codering_3': 'Code',
    'IndelingswijzigingGemeenteWijkBuurt_4': 'RegionChange',
    'AantalInwoners_5': 'Population',
    'k_0Tot15Jaar_8': 'Age_0_15',
    'k_15Tot25Jaar_9': 'Age_15_25',
    'k_25Tot45Jaar_10': 'Age_25_45',
    'k_45Tot65Jaar_11': 'Age_45_65',
    'k_65JaarOfOuder_12': 'Age_65plus',
    'HuishoudensTotaal_29': 'TotalHouseholds',
    'Eenpersoonshuishoudens_30': 'SinglePersonHouseholds',
    'HuishoudensZonderKinderen_31': 'HomeWithoutKids',
    'HuishoudensMetKinderen_32': 'HomeWithKids',
    'GemiddeldeHuishoudensgrootte_33': 'AvgHouseholdSize',
    'Bevolkingsdichtheid_34': 'PopulationDensity',
    'MeestVoorkomendePostcode_125': 'CommonPostcode',
    'Dekkingspercentage_126': 'CoveragePercent'
}, inplace=True)

# Strip whitespace from all string/object values
df = df.apply(lambda col: col.map(lambda x: x.strip() if isinstance(x, str) else x))

# Replace any '.' with np.nan
df.replace('.', np.nan, inplace=True)

# Convert numeric columns from object to float
cols_to_convert = ['AvgHouseholdSize', 'PopulationDensity', 'CoveragePercent']
df[cols_to_convert] = df[cols_to_convert].astype(float)

# Show missing values (optional)
print("Successfully converted. Missing values:\n", df[cols_to_convert].isna().sum())

df.info()

# Ensure all object columns are string
object_cols = df.select_dtypes(include='object').columns
df[object_cols] = df[object_cols].astype('string')

# Sort and pick top neighborhoods by population and density
top_pop = df[['Neighborhood', 'Population']].sort_values(by='Population', ascending=False).head(10)
top_density = df[['Neighborhood', 'PopulationDensity']].sort_values(by='PopulationDensity', ascending=False).head(10)

# Plot 1: Top neighborhoods by Population Density
plt.figure(figsize=(10,6))
plt.bar(top_density['Neighborhood'], top_density['PopulationDensity'], color='teal')
plt.xticks(rotation=45, ha='right')
plt.title('Top Neighborhoods by Population Density')
plt.ylabel('Population Density (per km²)')
plt.tight_layout()
plt.show()

# Plot 2: Top neighborhoods by Population (with density as second axis)
fig, ax1 = plt.subplots(figsize=(10,6))

color = 'tab:blue'
ax1.bar(top_pop['Neighborhood'], top_pop['Population'], color=color, label='Population')
ax1.set_ylabel('Population', color=color)
ax1.tick_params(axis='y', labelcolor=color)
ax1.set_xticklabels(top_pop['Neighborhood'], rotation=45, ha='right')

# Create second y-axis for density
ax2 = ax1.twinx()
color = 'tab:red'
pop_density = df.set_index('Neighborhood').loc[top_pop['Neighborhood'], 'PopulationDensity']
ax2.plot(top_pop['Neighborhood'], pop_density, color=color, marker='o', label='Population Density')
ax2.set_ylabel('Population Density (per km²)', color=color)
ax2.tick_params(axis='y', labelcolor=color)

plt.title('Top Neighborhoods by Population and Density')
fig.tight_layout()
plt.show()

# Plot 3: Histogram of Avg Household Size
plt.figure(figsize=(8,5))
df['AvgHouseholdSize'].dropna().plot(kind='hist', bins=15, color='skyblue', edgecolor='black')
plt.title('Distribution of Average Household Size')
plt.xlabel('Average Household Size')
plt.ylabel('Number of Neighborhoods')
plt.tight_layout()
plt.show()

# Ensure coding columns match for merge
housing_df['Coding'] = housing_df['Coding'].str.strip()
df['Code'] = df['Code'].str.strip()

# Merge the dataframes on neighborhood code
merged_df = pd.merge(housing_df, df, left_on='Coding', right_on='Code', how='inner')

# Drop duplicate key columns if necessary
merged_df.drop(columns=['Code', "CommonPostcode_y", "NeighborhoodChanges", "RegionType", "RegionChange", "CoveragePercent"], inplace=True)

# Quick check
print("✅ Merged dataframe shape:", merged_df.shape)
print(merged_df[['Neighborhood', 'Population', 'MeanWOZValueHome']].head())



# Convert MeanWOZValueHome to float
merged_df['MeanWOZValueHome'] = pd.to_numeric(merged_df['MeanWOZValueHome'], errors='coerce')

# Drop rows with missing WOZ values
df_model = merged_df.dropna(subset=['MeanWOZValueHome'])

# Select numerical & relevant features for prediction
features = [
    '%SingleFamilyHome', '%SemiDetached_SingleFamilyHome', '%CornerHouseSingleFamily',
    '%TwoUnderOneRoofHouseSingleFamily', '%DetachedHouseSingleFamilyHome', '%MultiFamilyHome',
    'OwnerOccupiedHome', 'RentalHouseTotal', 'BuiltLastTenYears',
    'Population', 'Age_25_45', 'Age_45_65', 'Age_65plus',
    'TotalHouseholds', 'AvgHouseholdSize', 'PopulationDensity'
]

# Convert features to float
for col in features:
    df_model.loc[:, col] = pd.to_numeric(df_model[col], errors='coerce')


# Drop any rows with remaining NaNs in predictors
df_model = df_model.dropna(subset=features)

# Define X and y
X = df_model[features]
y = df_model['MeanWOZValueHome']

"""## **BASELINE MODEL**
```
# This is formatted as code
```

# Model Without GM0855 and %DetachedHouseSingleFamilyHome
"""

# Remove GM0855 row (Tilburg-wide data)
df_model_reduced = df_model[df_model['Coding'] != 'GM0855'].copy()

# Define reduced feature list (excluding %DetachedHouseSingleFamilyHome)
reduced_features = [
    '%SingleFamilyHome', '%SemiDetached_SingleFamilyHome', '%CornerHouseSingleFamily',
    '%TwoUnderOneRoofHouseSingleFamily', '%MultiFamilyHome',
    'OwnerOccupiedHome', 'RentalHouseTotal', 'BuiltLastTenYears',
    'Population', 'Age_25_45', 'Age_45_65', 'Age_65plus',
    'TotalHouseholds', 'AvgHouseholdSize', 'PopulationDensity'
]

# Convert features to numeric
for col in reduced_features:
    df_model_reduced[col] = pd.to_numeric(df_model_reduced[col], errors='coerce')

# Drop any rows with NaNs
df_model_reduced = df_model_reduced.dropna(subset=reduced_features + ['MeanWOZValueHome'])

# Define X and y
X_reduced = df_model_reduced[reduced_features]
y_reduced = df_model_reduced['MeanWOZValueHome']

# Split data
X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(
    X_reduced, y_reduced, test_size=0.2, random_state=42
)

# Train model
rf_model_r = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model_r.fit(X_train_r, y_train_r)

# Predict
y_pred_r = rf_model_r.predict(X_test_r)

# Evaluate
mae_r = mean_absolute_error(y_test_r, y_pred_r)
rmse_r = np.sqrt(mean_squared_error(y_test_r, y_pred_r))
r2_r = r2_score(y_test_r, y_pred_r)

print("\n📉 Performance of Baseline Model:")
print(f"MAE: {mae_r:.2f}")
print(f"RMSE: {rmse_r:.2f}")
print(f"R² Score: {r2_r:.2f}")

# Feature importance plot
importances_r = rf_model_r.feature_importances_
sorted_idx_r = np.argsort(importances_r)[::-1]
sorted_features_r = X_reduced.columns[sorted_idx_r]

plt.figure(figsize=(10, 6))
plt.title("Feature Importances (Without %DetachedHouseSingleFamilyHome & GM0855)")
plt.bar(range(len(importances_r)), importances_r[sorted_idx_r])
plt.xticks(range(len(importances_r)), sorted_features_r, rotation=90)
plt.tight_layout()
plt.show()

# Plot actual vs predicted
plt.figure(figsize=(6, 5))
plt.scatter(y_test_r, y_pred_r, alpha=0.7)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
plt.xlabel("Actual WOZ Value (k€)")
plt.ylabel("Predicted WOZ Value (k€)")
plt.title("Actual vs Predicted WOZ (70/30 Split)")
plt.grid(True)
plt.tight_layout()
plt.show()

# Perform 5-fold cross-validation using neg_mean_squared_error
cv_scores = cross_val_score(
    rf_model_r, X, y,
    cv=5,
    scoring='neg_root_mean_squared_error'
)

# Convert scores to positive RMSE values
cv_rmse_scores = -cv_scores

print("Cross-Validation RMSE Scores:", np.round(cv_rmse_scores, 2))
print(f"Mean CV RMSE: {np.mean(cv_rmse_scores):.2f}")
print(f"Std Dev of CV RMSE: {np.std(cv_rmse_scores):.2f}")

# Setup KFold
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Prepare for loop
fold = 1
for train_idx, test_idx in kf.split(X):
    if fold == 5:  # We only care about Fold 5 here
        # Split data
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
        neighborhoods = df_model.iloc[test_idx]['Neighborhood'].values  # Get neighborhood names

        # Train model
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        # Evaluation
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))

        print(f"📦 Fold 5 MAE: {mae:.2f}")
        print(f"📦 Fold 5 RMSE: {rmse:.2f}")

        # DataFrame of results
        results = pd.DataFrame({
            'Neighborhood': neighborhoods,
            'Actual': y_test.values,
            'Predicted': y_pred,
            'Error': y_test.values - y_pred,
            'AbsoluteError': np.abs(y_test.values - y_pred)
        }).sort_values(by='AbsoluteError', ascending=False)

        print("\n📍 Top 5 Largest Errors in Fold 5:")
        print(results.head())

        # Plot: Actual vs Predicted
        plt.figure(figsize=(6, 5))
        plt.scatter(results['Actual'], results['Predicted'], alpha=0.7)
        plt.plot([results['Actual'].min(), results['Actual'].max()],
                 [results['Actual'].min(), results['Actual'].max()], 'r--')
        plt.xlabel("Actual WOZ Value (k€)")
        plt.ylabel("Predicted WOZ Value (k€)")
        plt.title("Fold 5: Actual vs Predicted WOZ")
        plt.grid(True)
        plt.tight_layout()
        plt.show()

        # Plot: Absolute Error Bar Chart
        plt.figure(figsize=(10, 5))
        results_sorted = results.sort_values(by='AbsoluteError', ascending=False)
        plt.bar(results_sorted['Neighborhood'], results_sorted['AbsoluteError'])
        plt.xticks(rotation=90)
        plt.ylabel("Absolute Error (k€)")
        plt.title("Absolute Prediction Error by Neighborhood (Fold 5)")
        plt.tight_layout()
        plt.show()

        break  # We only want Fold 5
    fold += 1

df_model[df_model['Neighborhood'].isin(['WK085560', 'BU08553801', 'BU08556614', 'BU08556603', 'BU08556708'])]





# Load and preview energy and sustainability data

energy_df = pd.read_csv(os.path.join(data_path, 'Energy & Sustainability.csv'), sep=";", index_col="ID")
energy_df.head()

energy_df.columns

# Assume df_energy is your sustainability dataset
energy_df = energy_df.rename(columns={
    'WijkenEnBuurten': 'Neighborhood',
    'SoortRegio_2': 'RegionType',
    'Codering_3': 'Coding',
    'GemiddeldeElektriciteitsleveringTotaal_48': 'AvgElectricity',
    'Appartement_49': 'Elec_Apartment',
    'Tussenwoning_50': 'Elec_RowHouse',
    'Hoekwoning_51': 'Elec_CornerHouse',
    'TweeOnderEenKapWoning_52': 'Elec_SemiDetached',
    'VrijstaandeWoning_53': 'Elec_Detached',
    'Huurwoning_54': 'Elec_Rental',
    'EigenWoning_55': 'Elec_Owned',
    'GemiddeldAardgasverbruikTotaal_56': 'AvgGas',
    'Appartement_57': 'Gas_Apartment',
    'Tussenwoning_58': 'Gas_RowHouse',
    'Hoekwoning_59': 'Gas_CornerHouse',
    'TweeOnderEenKapWoning_60': 'Gas_SemiDetached',
    'VrijstaandeWoning_61': 'Gas_Detached',
    'Huurwoning_62': 'Gas_Rental',
    'EigenWoning_63': 'Gas_Owned',
    'PercentageWoningenMetStadsverwarming_64': 'DistrictHeatingPercent',
    'MeestVoorkomendePostcode_120': 'CommonPostcode'
})

# Remove whitespace and convert numeric fields
energy_df = energy_df.applymap(lambda x: str(x).strip() if isinstance(x, str) else x)

# Convert appropriate columns to numeric
num_cols = energy_df.columns.difference(['Neighborhood', 'RegionType', 'Coding', 'CommonPostcode'])
energy_df[num_cols] = energy_df[num_cols].apply(pd.to_numeric, errors='coerce')

# Check for missing values
print(energy_df.isna().sum())

# Quick overview
print(energy_df.info())
energy_df.head()



# Merge energy data with main model on 'Coding'
df_model_energy = pd.merge(
    df_model,
    energy_df,
    how='left',
    left_on='Coding',
    right_on='Coding'
)

print("Merged shape:", df_model_energy.shape)

#Convert DistrictHeatingPercent to binary flag
df_model_energy['HasDistrictHeating'] = df_model_energy['DistrictHeatingPercent'].notnull().astype(int)

df_model_energy.head()

#Drop redundant columns
df_model_energy.info()

# Create new feature: Gas-to-Electricity Ratio
df_model_energy['GasPerElecRatio'] = df_model_energy['AvgGas'] / df_model_energy['AvgElectricity']

# Plot distributions
energy_cols = ['AvgElectricity', 'AvgGas', 'GasPerElecRatio']
plt.figure(figsize=(14, 4))
for i, col in enumerate(energy_cols):
    plt.subplot(1, 3, i+1)
    sns.histplot(df_model_energy[col], bins=30, kde=True, color='teal')
    plt.title(f'{col} Distribution')
plt.tight_layout()
plt.show()

# Correlation with WOZ
corrs = df_model_energy[['MeanWOZValueHome'] + energy_cols + ['HasDistrictHeating']].corr()
print("\n🔍 Correlation with WOZ Value:")
print(corrs['MeanWOZValueHome'].sort_values(ascending=False))

# Drop GM0855 (the city-level row)
df_model_energy = df_model_energy[df_model_energy['Coding'] != 'GM0855']

energy_features = [
    'AvgElectricity',       # Strongest correlation
    'AvgGas',               # Reasonable correlation
    'GasPerElecRatio',      # Efficiency / sustainability indicator
    'HasDistrictHeating'    # Urban heating infrastructure
]

# 🎯 Step 1: Define refined feature set
selected_features = [
    'AvgElectricity', 'OwnerOccupiedHome', 'RentalHouseTotal',
    'BuiltLastTenYears', '%TwoUnderOneRoofHouseSingleFamily',
    'AvgGas', 'AvgHouseholdSize', '%SemiDetached_SingleFamilyHome', 'Age_65plus'
]

# Low-impact features to drop
low_impact_features = [
    '%SingleFamilyHome', 'Population', '%MultiFamilyHome',
    'TotalHouseholds', '%CornerHouseSingleFamily'
]

# Drop GM0855 row
df_filtered = df_model_energy[df_model_energy['Coding'] != 'GM0855'].copy()

# Define target and features
target = 'MeanWOZValueHome'
features = [
    col for col in df_filtered.columns
    if col not in low_impact_features + ['Coding', target]
]

# Drop non-numeric columns (before imputation)
X = df_filtered[features].select_dtypes(include=[np.number]).copy()
y = pd.to_numeric(df_filtered[target], errors='coerce')

# Drop target NA
valid_idx = y.notna()
X = X.loc[valid_idx]
y = y.loc[valid_idx]

# Impute missing values
imputer = SimpleImputer(strategy='median')
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, random_state=42
)

# Train XGBoost
xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)
xgb_model.fit(X_train, y_train.to_numpy())

# Evaluate
y_pred = xgb_model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\n Cleaned XGBoost Model Performance (with Imputation):")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R² Score: {r2:.2f}")

# SHAP Summary
explainer = shap.Explainer(xgb_model, X_train)
shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test, plot_type="bar")
shap.summary_plot(shap_values, X_test)





# Load and preview income and Socioeconomic data

income_df = pd.read_csv(os.path.join(data_path, 'Income.csv'), sep=";", index_col="ID")
income_df.head()

income_df.columns

# Define and drop unnecessary columns first
columns_to_drop = [
    "SoortRegio_2", "Nettoarbeidsparticipatie_73",
    "PercentageWerknemers_74", "PercentageZelfstandigen_75"
]

income_df.drop(columns=columns_to_drop, inplace=True, errors="ignore")

#  Rename columns for readability
income_df.columns = [
    "Neighborhood", "Coding", "IncomeReceivers",
    "AvgIncomePerReceiver", "AvgIncomePerResident",
    "PctLowest40Incomes", "PctTop20Incomes", "AvgStandardizedIncome",
    "PctLowest40HouseholdsIncome", "PctTop20HouseholdsIncome",
    "LowIncomeHouseholds", "BelowOrAroundSocialMinimum",
    "Below110PctSocialMin", "Below120PctSocialMin",
    "MedianWealth", "CommonPostcode"
]

# Replace '.' with NaN and convert types
income_df.replace(".", np.nan, inplace=True)
for col in income_df.columns:
    if income_df[col].dtype == "object":
        income_df[col] = income_df[col].map(lambda x: str(x).strip() if isinstance(x, str) else x)

# Convert all numeric-looking columns to floats
for col in income_df.columns[2:]:
    income_df[col] = pd.to_numeric(income_df[col], errors="coerce")

# Preview structure
print("\n🔍 Missing values:")
print(income_df.isna().sum())

income_df.info()

# Drop high-missing columns
cols_to_drop = [
    'AvgIncomePerReceiver',
    'AvgIncomePerResident',
    'AvgStandardizedIncome'
]
income_cleaned = income_df.drop(columns=cols_to_drop)

# Impute remaining numeric columns with median
imputer = SimpleImputer(strategy='median')
numeric_cols = income_cleaned.select_dtypes(include=[np.number]).columns
income_cleaned[numeric_cols] = imputer.fit_transform(income_cleaned[numeric_cols])

# 👀 Final check
print("\n✅ Cleaned income data:")
print(income_cleaned.info())
income_cleaned.head(3)

# Drop Tilburg-wide aggregate
income_cleaned = income_cleaned[income_cleaned['Coding'] != 'GM0855'].copy()

# Plot histograms for all numeric income features
numeric_cols = income_cleaned.select_dtypes(include='number').columns.drop(['CommonPostcode'])

income_cleaned[numeric_cols].hist(
    figsize=(15, 12),
    bins=30,
    edgecolor='black',
    layout=(4, 3)
)
plt.suptitle("Distributions of Income & Socioeconomic Features", fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()







#  Merge df_filtered and income_cleaned on 'Coding'
df_merged = pd.merge(
    df_filtered,
    income_cleaned,
    on='Coding',
    how='left',
    suffixes=('', '_income')
)

#  Drop unwanted columns
low_impact_features = [
    '%SingleFamilyHome', 'Population', '%MultiFamilyHome',
    'TotalHouseholds', '%CornerHouseSingleFamily', 'LowIncomeHouseholds',
    'BelowOrAroundSocialMinimum', 'AvgGas', 'AvgHouseholdSize',
    'PctLowest40Incomes', 'GasRental', 'GasDetached', 'Age_25_45',
    'HomeWithKids', 'GasPerElecRatio', 'Gas_Apartment', 'Gas_Owned',
    'Gas_CornerHouse'
]
exclude_cols = ['Coding', 'MeanWOZValueHome'] + low_impact_features
target = 'MeanWOZValueHome'

#  Select numeric features only
X = df_merged.drop(columns=exclude_cols, errors='ignore').select_dtypes(include=[np.number])
y = pd.to_numeric(df_merged[target], errors='coerce')

#  Handle missing target values
valid_idx = y.notna()
X = X.loc[valid_idx]
y = y.loc[valid_idx]

# Impute missing values (median)
imputer = SimpleImputer(strategy='median')
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# 🧪 STEP 6: Split into train/test
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, random_state=42
)

#  Train XGBoost
xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)
xgb_model.fit(X_train, y_train.to_numpy())

#  Evaluate
y_pred = xgb_model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\n📊 XGBoost Model Performance with Income Data:")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R² Score: {r2:.2f}")


# SHAP Interpretation
explainer = shap.Explainer(xgb_model, X_train)
shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test, plot_type="bar")
shap.summary_plot(shap_values, X_test)

corrs = X.corrwith(y).sort_values(ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(x=corrs.values[:15], y=corrs.index[:15])
plt.title("Top Correlated Features with WOZ Value")
plt.xlabel("Absolute Correlation with WOZ Value")  # ➕ X-axis label
plt.ylabel("Feature Name")                         # ➕ Y-axis label
plt.tight_layout()
plt.show()



# Load and preview Geo Spatial data

geospatial_df = pd.read_csv(os.path.join(data_path, 'Geo_Spatial.csv'), sep=";", index_col="ID")
geospatial_df.head()

# Rename columns in geospatial_df for clarity
geospatial_df.rename(columns={
    'WijkenEnBuurten': 'Neighborhood',
    'SoortRegio_2': 'RegionType',
    'Codering_3': 'Coding',
    'PersonenautoSPerHuishouden_109': 'CarsPerHousehold',
    'PersonenautoSNaarOppervlakte_110': 'CarsPerArea',
    'AfstandTotHuisartsenpraktijk_112': 'DistanceToGP',
    'AfstandTotGroteSupermarkt_113': 'DistanceToSupermarket',
    'AfstandTotKinderdagverblijf_114': 'DistanceToDaycare',
    'ScholenBinnen3Km_116': 'SchoolsWithin3km',
    'OppervlakteTotaal_117': 'TotalArea',
    'OppervlakteLand_118': 'LandArea',
    'OppervlakteWater_119': 'WaterArea',
    'MeestVoorkomendePostcode_120': 'CommonPostcode',
    'Dekkingspercentage_121': 'CoveragePercent',
    'MateVanStedelijkheid_122': 'UrbanityLevel',
    'Omgevingsadressendichtheid_123': 'AddressDensity'
}, inplace=True)

geospatial_df.columns

geospatial_df.info()

# Copy to avoid modifying original
geo_cleaned = geospatial_df.copy()

# Replace "." placeholders with real NaNs
geo_cleaned = geo_cleaned.replace(".", np.nan)

# Drop GM0855 row (Citywide data)
geo_cleaned = geo_cleaned[geo_cleaned['Coding'] != 'GM0855'].copy()

# Convert columns to numeric where needed
numeric_cols = [
    'CarsPerHousehold', 'CarsPerArea', 'DistanceToGP', 'DistanceToSupermarket',
    'DistanceToDaycare', 'SchoolsWithin3km', 'CommonPostcode',
    'CoveragePercent', 'UrbanityLevel', 'AddressDensity'
]
for col in numeric_cols:
    geo_cleaned[col] = pd.to_numeric(geo_cleaned[col], errors='coerce')

# Check cleaned data
print("\n Cleaned GeoSpatial Data:")
display(geo_cleaned.head())
display(geo_cleaned.dtypes)

# Remove Tilburg-wide row
geo_cleaned = geo_cleaned[geo_cleaned['Coding'] != 'GM0855'].copy()

# Drop unnecessary columns
geo_cleaned = geo_cleaned.drop(columns=['RegionType', 'CommonPostcode'])

# Replace '.' with real NaN if any remained (precaution)
geo_cleaned.replace('.', np.nan, inplace=True)

# Convert all necessary columns to numeric
for col in geo_cleaned.columns:
    if col not in ['Neighborhood', 'Coding']:  # Only exclude identifiers
        geo_cleaned[col] = pd.to_numeric(geo_cleaned[col], errors='coerce')

# Check missing values
print("\n Missing values per column BEFORE imputation:")
display(geo_cleaned.isna().sum())



imputer = SimpleImputer(strategy='median')
geo_imputed = pd.DataFrame(
    imputer.fit_transform(geo_cleaned.select_dtypes(include=[np.number])),
    columns=geo_cleaned.select_dtypes(include=[np.number]).columns
)

# Reattach Coding (non-numeric columns)
geo_imputed['Coding'] = geo_cleaned['Coding'].values

# Set index (optional)
geo_imputed.set_index(geo_cleaned.index, inplace=True)

# Final Cleaned GeoSpatial Data
print("\n Cleaned and Imputed GeoSpatial Data ready for merge:")
display(geo_imputed.head())

# Drop GM0855 from geospatial features
geo_final = geo_imputed[geo_imputed['Coding'] != 'GM0855'].copy()

#  Merge geospatial data
full_df = pd.merge(
    df_merged,
    geo_final,
    on='Coding',
    how='left'
)

# Check the result
print(f"✅ Shape after merging GeoSpatial features: {full_df.shape}")
display(full_df.head())

full_df.columns



#  List of columns to permanently drop
columns_to_drop = [
    '%SingleFamilyHome', 'Population', '%MultiFamilyHome', 'TotalHouseholds',
    '%CornerHouseSingleFamily', 'LowIncomeHouseholds', 'BelowOrAroundSocialMinimum',
    'AvgGas', 'AvgHouseholdSize', 'PctLowest40Incomes', 'GasRental',
    'GasDetached', 'Age_25_45', 'HomeWithKids', 'GasPerElecRatio',
    'Gas_Apartment', 'Gas_Owned', 'Gas_CornerHouse',
    'CommonPostcode', 'CommonPostcode_x', 'CommonPostcode_income',
    'Neighborhood', 'Neighborhood_x', 'Neighborhood_y', 'RegionType',
    'Coverage%', 'CoveragePercent', 'HousingStock'
]

# 🚀 Permanently drop the columns
full_df.drop(columns=columns_to_drop, axis=1, inplace=True, errors='ignore')

print(f" Columns dropped! New shape: {full_df.shape}")

full_df.info()

# Convert object columns to numeric
object_cols = [
    '%SemiDetached_SingleFamilyHome', '%TwoUnderOneRoofHouseSingleFamily',
    '%DetachedHouseSingleFamilyHome', 'UninhabitedHome', 'OwnerOccupiedHome',
    'RentalHouseTotal', 'OwnedByHouseCorporation', 'OtherOwners',
    'BuiltLastTenYears', 'HomeWithoutKids'
]

for col in object_cols:
    full_df[col] = pd.to_numeric(full_df[col], errors='coerce')

# Handle missing values (median imputation)

# Select only numeric columns
X = full_df.drop(columns=['Coding', 'MeanWOZValueHome'])  # Drop non-features
y = full_df['MeanWOZValueHome']

# Imputer for features
imputer = SimpleImputer(strategy='median')
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# Train/test split

X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, random_state=42
)

# Train XGBoost

xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)

xgb_model.fit(X_train, y_train)

# Evaluate
y_pred = xgb_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\n Final Cleaned XGBoost Model Performance:")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R² Score: {r2:.2f}")

#SHAP Explanation

explainer = shap.Explainer(xgb_model, X_train)
shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test, plot_type="bar")
shap.summary_plot(shap_values, X_test)

# Only numerical features
corr_matrix = full_df.select_dtypes(include=[np.number]).corr()

# Plot
plt.figure(figsize=(14, 10))
sns.heatmap(corr_matrix, cmap='coolwarm', center=0, annot=False)
plt.title("🔍 Correlation Matrix of Features", fontsize=16)
plt.show()



"""Adding the GeoSpatial features didn't improve my model performance, removing most of it as I did below didn't improve it either as I already had similar performance before. I will add the distance to fire staion data to see how it affects my model performance

"""

# 🔥 Step 1: Define columns to DROP
geo_columns_to_drop = [
    'DistanceToGP', 'DistanceToSupermarket', 'DistanceToDaycare', 'SchoolsWithin3km',
    'TotalArea', 'LandArea', 'WaterArea', 'CoveragePercent'
]

# 🔥 Step 2: Drop these columns permanently
full_df.drop(columns=geo_columns_to_drop, inplace=True, errors='ignore')

print(f"✅ Shape after dropping unnecessary geospatial columns: {full_df.shape}")

# 🔥 Step 3: Continue with your usual model building
# (same as before — define X, y, impute missing values, train XGBoost, evaluate)

# Example:

# Define features and target
target = 'MeanWOZValueHome'
exclude_cols = ['Coding', target]
X = full_df.drop(columns=exclude_cols, errors='ignore').select_dtypes(include=[np.number])
y = pd.to_numeric(full_df[target], errors='coerce')

# Drop missing target rows
valid_idx = y.notna()
X = X.loc[valid_idx]
y = y.loc[valid_idx]

# Impute missing features
imputer = SimpleImputer(strategy='median')
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, random_state=42
)

# Train XGBoost
xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)
xgb_model.fit(X_train, y_train.to_numpy())

# Predict
y_pred = xgb_model.predict(X_test)

# Evaluate
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # fixed the squared=False mistake
r2 = r2_score(y_test, y_pred)

print("\n📊 New Cleaned XGBoost Model Performance:")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R² Score: {r2:.2f}")

# SHAP
explainer = shap.Explainer(xgb_model, X_train)
shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test, plot_type="bar")
shap.summary_plot(shap_values, X_test)





#  Load the file containing distance to fire station
fire_df = pd.read_csv(
    os.path.join(data_path, 'proximity85830NED_UntypedDataSet_22032025_101506.csv'),
    sep=";", index_col="ID"
)

fire_df.tail()

fire_df.info()

# 1. Select only the useful columns
fire_features = fire_df[[
    'Codering_3',
    'AfstandTotBrandweerkazerne_114',
    'AfstandTotTreinstationsTotaal_90',
    'AfstandTotBelangrijkOverstapstation_91'
]].copy()

# 2. Rename columns
fire_features.rename(columns={
    'Codering_3': 'Coding',
    'AfstandTotBrandweerkazerne_114': 'DistanceToFireStation',
    'AfstandTotTreinstationsTotaal_90': 'DistanceToTrainStation',
    'AfstandTotBelangrijkOverstapstation_91': 'DistanceToMajorTransferStation'
}, inplace=True)

# 3. Replace '.' with NaN and convert columns to numeric
for col in fire_features.columns[1:]:
    fire_features[col] = fire_features[col].replace('.', np.nan)
    fire_features[col] = pd.to_numeric(fire_features[col], errors='coerce')

# 4. Drop any rows where 'Coding' is missing (if any)
fire_features = fire_features.dropna(subset=['Coding'])

# 5. Final Check
print(f"✅ Fire features final shape (after excluding bad columns): {fire_features.shape}")
display(fire_features.head())



"""As seen below, adding distance to fire service station didn't improve model performance either."""

# 🛠️ Merge fire station features into full_df
# (Remember: drop GM0855 ('Coding' == 'GM0855') if needed)

# 1. Merge on 'Coding'
final_df = pd.merge(
    full_df, fire_features,
    on='Coding',
    how='left'   # left join to preserve your modeling rows
)

# 2. Define target and features
target = 'MeanWOZValueHome'
X = final_df.drop(columns=[target, 'Coding'], errors='ignore')
y = final_df[target]

# 3. Convert object types to numeric where necessary
X = X.apply(pd.to_numeric, errors='coerce')

# 4. Impute missing values with median
imputer = SimpleImputer(strategy='median')
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# 5. Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, random_state=42
)

# 6. Train XGBoost model
xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)
xgb_model.fit(X_train, y_train)

# 7. Predict and evaluate
y_pred = xgb_model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\n📊 Final Model Performance (with Fire Station Proximity):")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R² Score: {r2:.2f}")

# 8. SHAP Interpretation
explainer = shap.Explainer(xgb_model, X_train)
shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test, plot_type="bar")
shap.summary_plot(shap_values, X_test)





"""I tried a model with variables strictly describing houses or it's loction. Not those describing people that live in it to compare performance below.

"""

selected_features = [
    '%SemiDetached_SingleFamilyHome',
    '%DetachedHouseSingleFamilyHome',
    'BuiltLastTenYears',
    'PopulationDensity',
    #'DistanceToGP',
    #'DistanceToSupermarket',
    'DistanceToTrainStation',
    'DistanceToFireStation',   # 🆕
    'UrbanityLevel',
    'AddressDensity'
]



final_df.columns

# Add DistanceToFireStation and build the model

X_small = final_df[selected_features].copy()
y_small = final_df['MeanWOZValueHome'].copy()

# Impute missing values
imputer = SimpleImputer(strategy='median')
X_small_imputed = pd.DataFrame(imputer.fit_transform(X_small), columns=X_small.columns)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X_small_imputed, y_small, test_size=0.2, random_state=42)

# Train simple XGBoost
xgb_small = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)
xgb_small.fit(X_train, y_train.to_numpy())

# Predict and evaluate
y_pred = xgb_small.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\n📊 Physical Feature XGBoost Model Performance (with DistanceToFireStation):")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R² Score: {r2:.2f}")

# Shap analysis of physical factors on the prediction
explainer_simple = shap.Explainer(xgb_small, X_train)
shap_values_simple = explainer_simple(X_test)

shap.summary_plot(shap_values_simple, X_test, plot_type="bar")
shap.summary_plot(shap_values_simple, X_test)



# Random forest on physical features
# Random Forest Regressor
rf_model_small = RandomForestRegressor(n_estimators=200, random_state=42)
rf_model_small.fit(X_train, y_train)

# Predict
y_pred_rf = rf_model_small.predict(X_test)

# Metrics
mae_rf = mean_absolute_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)

print("\n🌲 Random Forest Performance (Physical Features Only):")
print(f"MAE: {mae_rf:.2f}")
print(f"RMSE: {rmse_rf:.2f}")
print(f"R² Score: {r2_rf:.2f}")



# Base models for stacking
base_models = [
    ('rf', RandomForestRegressor(n_estimators=200, random_state=42)),
    ('ridge', Ridge(alpha=1.0))
]

# Final model
final_model = Ridge(alpha=1.0)

# Stacking Regressor
stacked_model = StackingRegressor(
    estimators=base_models,
    final_estimator=final_model,
    passthrough=True,
    n_jobs=-1
)

# Train
stacked_model.fit(X_train, y_train)

# Predict
y_pred_stack = stacked_model.predict(X_test)

# Metrics
mae_stack = mean_absolute_error(y_test, y_pred_stack)
rmse_stack = np.sqrt(mean_squared_error(y_test, y_pred_stack))
r2_stack = r2_score(y_test, y_pred_stack)

print("\n🧩 Stacked Model (RandomForest + Ridge) Performance (Physical Features Only):")
print(f"MAE: {mae_stack:.2f}")
print(f"RMSE: {rmse_stack:.2f}")
print(f"R² Score: {r2_stack:.2f}")



# Only use the RandomForest inside the stacker for SHAP
rf_model = stacked_model.named_estimators_['rf']

# Create SHAP explainer
explainer = shap.Explainer(rf_model, X_test)
shap_values = explainer(X_test)

# Bar plot (mean SHAP value)
shap.summary_plot(shap_values, X_test, plot_type="bar")

# Beeswarm (detailed plot)
shap.summary_plot(shap_values, X_test)

#  your correlation matrix

X_small_imputed_with_target = X_small_imputed.copy()
X_small_imputed_with_target['MeanWOZValueHome'] = y_small.values

# Plot correlation including target
plt.figure(figsize=(12, 10))
corr_matrix = X_small_imputed_with_target.corr()

sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, fmt=".2f", square=True, cbar_kws={"shrink": .8})
plt.title("🔵 Correlation Matrix (Physical Features + Target: MeanWOZValueHome)")
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()



# Predict again if needed
y_pred_rf = rf_model.predict(X_test)
y_pred_stacked = stacked_model.predict(X_test)

# Create a DataFrame for comparing true vs predicted values

comparison_df = pd.DataFrame({
    'Actual': y_test,
    'RandomForest_Predicted': y_pred_rf,
    'StackedModel_Predicted': y_pred_stacked
})

# 2. Heatmap of Errors
plt.figure(figsize=(8, 6))
error_corr = comparison_df.corr()

sns.heatmap(error_corr, cmap='coolwarm', annot=True, fmt=".2f", square=True, cbar_kws={"shrink": .8})
plt.title("🔵 Correlation of Actual vs Model Predictions")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()



# Final model results
model_results = {
    'Model': [
        'XGBoost (Full)',
        'Voting (RF + XGB)',
        'XGBoost (Physical Only)',
        'Random Forest (Physical Only)',
        'Stacked Model (Physical Only)',
        'LightGBM (Full)',
        'Random Forest (Baseline)'
    ],
    'MAE': [21.94, 21.94, 31.90, 45.69, 43.13, 27.21, 33.40],
    'RMSE': [31.99, 30.85, 63.69, 65.83, 58.91, 38.49, 52.82],
    'R2': [0.94, 0.95, 0.77, 0.75, 0.79, 0.80, 0.91]
}

df_results = pd.DataFrame(model_results)

# Plotting
fig, axes = plt.subplots(3, 1, figsize=(12, 18))

# MAE Plot
axes[0].barh(df_results['Model'], df_results['MAE'], color='skyblue')
axes[0].set_title('MAE Comparison Across Models')
axes[0].invert_yaxis()
axes[0].set_xlabel('Mean Absolute Error (lower is better)')

# RMSE Plot
axes[1].barh(df_results['Model'], df_results['RMSE'], color='lightgreen')
axes[1].set_title('RMSE Comparison Across Models')
axes[1].invert_yaxis()
axes[1].set_xlabel('Root Mean Squared Error (lower is better)')

# R2 Score Plot
axes[2].barh(df_results['Model'], df_results['R2'], color='salmon')
axes[2].set_title('R² Score Comparison Across Models')
axes[2].invert_yaxis()
axes[2].set_xlabel('R² Score (higher is better)')

plt.tight_layout()
plt.show()









# ✅ Merge fire_features to df_merged
df_fire_merged = df_merged.merge(fire_features, on='Coding', how='left')

# ✅ Drop the low-impact features
low_impact_features = [
    '%SingleFamilyHome', 'Population', '%MultiFamilyHome',
    'TotalHouseholds', '%CornerHouseSingleFamily', 'LowIncomeHouseholds',
    'BelowOrAroundSocialMinimum', 'AvgGas', 'AvgHouseholdSize',
    'PctLowest40Incomes', 'GasRental', 'GasDetached', 'Age_25_45',
    'HomeWithKids', 'GasPerElecRatio', 'Gas_Apartment', 'Gas_Owned',
    'Gas_CornerHouse'
]

target = 'MeanWOZValueHome'
exclude_cols = ['Coding', target] + low_impact_features

# 📦 Select numeric features
X = df_fire_merged.drop(columns=exclude_cols, errors='ignore').select_dtypes(include=[np.number])
y = pd.to_numeric(df_fire_merged[target], errors='coerce')

# 🛠️ Drop rows with missing target
valid_idx = y.notna()
X = X.loc[valid_idx]
y = y.loc[valid_idx]

# 🧽 Impute missing values
imputer = SimpleImputer(strategy='median')
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# 🔀 Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, random_state=42
)

# 🚀 Train model
xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)
xgb_model.fit(X_train, y_train.to_numpy())

# 📈 Evaluate
y_pred = xgb_model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\n📊 Model Performance (Fire Station added to df_merged, no GeoSpatial):")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R² Score: {r2:.2f}")

# 📊 SHAP analysis (optional)
explainer = shap.Explainer(xgb_model, X_train)
shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test, plot_type="bar")
shap.summary_plot(shap_values, X_test)

"""Let's try other algorithms besides just XGBoost that have been used so far. Maybe even an emsembled method

"""

# Train RandomForest
rf_model = RandomForestRegressor(
    n_estimators=200,
    max_depth=8,
    random_state=42
)
rf_model.fit(X_train, y_train)

# Predict
rf_pred = rf_model.predict(X_test)

# Evaluate
rf_mae = mean_absolute_error(y_test, rf_pred)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))
rf_r2 = r2_score(y_test, rf_pred)

print("\n🌲 Random Forest Performance:")
print(f"MAE: {rf_mae:.2f}")
print(f"RMSE: {rf_rmse:.2f}")
print(f"R² Score: {rf_r2:.2f}")

# Train LightGBM
lgb_model = LGBMRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=8,
    random_state=42
)
lgb_model.fit(X_train, y_train)

# Predict
lgb_pred = lgb_model.predict(X_test)

# Evaluate
lgb_mae = mean_absolute_error(y_test, lgb_pred)
lgb_rmse = np.sqrt(mean_squared_error(y_test, lgb_pred))
lgb_r2 = r2_score(y_test, lgb_pred)

print("\n⚡ LightGBM Performance:")
print(f"MAE: {lgb_mae:.2f}")
print(f"RMSE: {lgb_rmse:.2f}")
print(f"R² Score: {lgb_r2:.2f}")

# Define the base models
rf_model = RandomForestRegressor(
    n_estimators=200,
    max_depth=6,
    random_state=42
)

xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)

# Fit the models individually first
rf_model.fit(X_train, y_train)
xgb_model.fit(X_train, y_train)

# Create a Voting Regressor
voting_model = VotingRegressor(
    estimators=[
        ('rf', rf_model),
        ('xgb', xgb_model)
    ]
)

# Fit the Voting Regressor
voting_model.fit(X_train, y_train)

# Predict and Evaluate
y_pred = voting_model.predict(X_test)

# Metrics
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\n🎯 Voting Regressor (RandomForest + XGBoost) Performance:")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R² Score: {r2:.2f}")





"""Test model scalability with data from Breda

"""

breda_df = pd.read_csv(os.path.join(data_path, 'Breda_data.csv'), sep=";", index_col="ID")
breda_df.head()

breda_df.info()

breda_df.shape

breda_df_2 = pd.read_csv(os.path.join(data_path, 'Breda_data_2.csv'), sep=";", index_col="ID")
breda_df_2.head()

breda_df_2.info()

breda_df_2.head()

# Exclude "GM0758" (the general Breda municipality row)
breda_df = breda_df[~breda_df['Codering_3'].str.startswith('GM')]
breda_df_2 = breda_df_2[~breda_df_2['Codering_3'].str.startswith('GM')]

# Select useful columns from breda_df
breda_main = breda_df[[
    'Codering_3',                     # For merging
    'GemiddeldeWOZWaardeVanWoningen_36', # Mean WOZ Value
    'PercentageEengezinswoning_37',
    'TweeOnderEenKapWoning_52',
    'VrijstaandeWoning_53',
    'BouwjaarVanaf2000_47',
    'Omgevingsadressendichtheid_123',  # Population Density
    'MateVanStedelijkheid_122',        # Urbanity Level
    'AfstandTotGroteSupermarkt_113'    # Distance to Supermarket
]].copy()

# Select only useful and clean columns from breda_df_2
# Check for columns that don't have weird "." entries
columns_to_keep = [
    'Codering_3',
    'AfstandTotHuisartsenpraktijk_5',  # Distance to GP
    'AfstandTotTreinstationsTotaal_90', # Distance to Train Station
    'AfstandTotBrandweerkazerne_114'    # Distance to Fire Station
]
breda_proximity = breda_df_2[columns_to_keep].copy()

# Rename columns to match Tilburg format
breda_main.rename(columns={
    'Codering_3': 'Coding',
    'GemiddeldeWOZWaardeVanWoningen_36': 'MeanWOZValueHome',
    'PercentageEengezinswoning_37': '%SingleFamilyHome',
    'TweeOnderEenKapWoning_52': '%SemiDetached_SingleFamilyHome',
    'VrijstaandeWoning_53': '%DetachedHouseSingleFamilyHome',
    'BouwjaarVanaf2000_47': 'BuiltLastTenYears',
    'Omgevingsadressendichtheid_123': 'PopulationDensity',
    'MateVanStedelijkheid_122': 'UrbanityLevel',
    'AfstandTotGroteSupermarkt_113': 'DistanceToSupermarket'
}, inplace=True)

breda_proximity.rename(columns={
    'Codering_3': 'Coding',
    'AfstandTotHuisartsenpraktijk_5': 'DistanceToGP',
    'AfstandTotTreinstationsTotaal_90': 'DistanceToTrainStation',
    'AfstandTotBrandweerkazerne_114': 'DistanceToFireStation'
}, inplace=True)

# Merge the two cleaned datasets
merged_breda = pd.merge(breda_main, breda_proximity, on='Coding', how='inner')

# Convert all columns except 'Coding' to numeric
for col in merged_breda.columns:
    if col != 'Coding':
        merged_breda[col] = pd.to_numeric(merged_breda[col], errors='coerce')

# Handle missing values if necessary (for example, using median imputation later)
# merged_breda = merged_breda.fillna(merged_breda.median())

#  Check final dataset
print("✅ Final Breda Cleaned Shape:", merged_breda.shape)
display(merged_breda.head())

merged_breda.info()



"""Retrain XGBoost again on features describing houses which also appear in the Breda data

"""

selected_features = [
    '%SemiDetached_SingleFamilyHome',
    '%DetachedHouseSingleFamilyHome',
    'BuiltLastTenYears',
    'PopulationDensity',
    'DistanceToTrainStation',
    'DistanceToFireStation',
    'UrbanityLevel'
]

# X = final_df[selected_features]
X_small = final_df[selected_features].copy()
y_small = final_df['MeanWOZValueHome'].copy()

# Impute missing values if needed
imputer = SimpleImputer(strategy='median')
X_small_imputed = pd.DataFrame(imputer.fit_transform(X_small), columns=X_small.columns)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X_small_imputed, y_small, test_size=0.2, random_state=42)

# Train XGBoost
xgb_small = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)
xgb_small.fit(X_train, y_train)

# Predict on Tilburg test
y_pred = xgb_small.predict(X_test)
print(f"Train/Test MAE: {mean_absolute_error(y_test, y_pred):.2f}")
print(f"Train/Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")
print(f"Train/Test R²: {r2_score(y_test, y_pred):.2f}")

"""Then predict on Breda below"""

# Remove rows where Breda actual WOZ is missing
breda_valid = merged_breda.dropna(subset=['MeanWOZValueHome'])

X_breda_valid = breda_valid[selected_features]
y_breda_actual = breda_valid['MeanWOZValueHome']

# Impute X (features)
X_breda_imputed = pd.DataFrame(imputer.transform(X_breda_valid), columns=X_breda_valid.columns)

# Predict
y_breda_pred = xgb_small.predict(X_breda_imputed)

# Now evaluate!
print("\n📊 Breda Test Results:")
print(f"MAE: {mean_absolute_error(y_breda_actual, y_breda_pred):.2f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_breda_actual, y_breda_pred)):.2f}")
print(f"R²: {r2_score(y_breda_actual, y_breda_pred):.2f}")

print(breda_valid[selected_features].describe())

print(final_df[selected_features].describe())



"""The performance of the model on Breda data is not proper and the key observations are:

1. %SemiDetached and %Detached look totally wrong in Breda	❗ Values like 3368, 4275 are not percentages but seem like absolute electricity consumption or another metric.
2. Population Density is very different	5835 (Tilburg) vs 1877 (Breda) → ⚠️ Model trained on much denser area.
3. Other distance features (fire station, train) look okayish!

So, Retrain model on Tilburg without using %SemiDetached and %DetachedHouse (to simulate fair comparison).

Test again on Breda (only use "healthy" features).


"""

# Selected features
selected_clean_features = [
    'BuiltLastTenYears',
    'PopulationDensity',
    'DistanceToTrainStation',
    'DistanceToFireStation',
    'UrbanityLevel'
]

# Prepare train (Tilburg)
X_small = final_df[selected_clean_features].copy()
y_small = final_df['MeanWOZValueHome'].copy()

# Impute missing values for Tilburg
imputer = SimpleImputer(strategy='median')
X_small_imputed = pd.DataFrame(imputer.fit_transform(X_small), columns=X_small.columns)

# Train/test split on Tilburg
X_train, X_test, y_train, y_test = train_test_split(X_small_imputed, y_small, test_size=0.2, random_state=42)

# Retrain model
xgb_small_clean = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)
xgb_small_clean.fit(X_train, y_train)

# Evaluate on Tilburg
y_pred_train = xgb_small_clean.predict(X_test)

print("\n📊 Cleaned Tilburg Model Performance (internal):")
print(f"MAE: {mean_absolute_error(y_test, y_pred_train):.2f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_train)):.2f}")
print(f"R² Score: {r2_score(y_test, y_pred_train):.2f}")

# Prepare test (Breda)

# Drop Breda rows where MeanWOZValueHome is missing
merged_breda_clean = merged_breda.dropna(subset=['MeanWOZValueHome']).copy()

# Drop Breda rows where feature values are missing
merged_breda_clean = merged_breda_clean.dropna(subset=selected_clean_features).copy()

print(f"\n✅ Breda cleaned shape for testing: {merged_breda_clean.shape}")

# Prepare Breda input
X_breda = merged_breda_clean[selected_clean_features].copy()
y_breda_actual = merged_breda_clean['MeanWOZValueHome'].copy()

# Impute Breda (use the same imputer trained on Tilburg)
X_breda_imputed = pd.DataFrame(imputer.transform(X_breda), columns=X_breda.columns)

# Predict on Breda
y_breda_pred = xgb_small_clean.predict(X_breda_imputed)

print("\n📊 Breda Test Results (Cleaned Model):")
print(f"MAE: {mean_absolute_error(y_breda_actual, y_breda_pred):.2f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_breda_actual, y_breda_pred)):.2f}")
print(f"R² Score: {r2_score(y_breda_actual, y_breda_pred):.2f}")

# Scatter plot: Actual vs Predicted
plt.figure(figsize=(8, 6))
plt.scatter(y_breda_actual, y_breda_pred, alpha=0.7)
plt.plot([y_breda_actual.min(), y_breda_actual.max()],
         [y_breda_actual.min(), y_breda_actual.max()],
         'r--', lw=2)
plt.xlabel('Actual WOZ Value (Breda)')
plt.ylabel('Predicted WOZ Value (Breda)')
plt.title('📈 Actual vs Predicted - Breda Test Set')
plt.grid(True)
plt.show()

# Residual plot: Errors
residuals = y_breda_actual - y_breda_pred

plt.figure(figsize=(8, 6))
plt.scatter(y_breda_actual, residuals, alpha=0.7)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('Actual WOZ Value (Breda)')
plt.ylabel('Residuals (Actual - Predicted)')
plt.title('📉 Residual Plot - Breda Test Set')
plt.grid(True)
plt.show()





"""Clean data from Delft, Den Bosch, Dordrecht, Eindhoven and Nijmegen and text my model on it for scalability"""

DDDEN_proximity = pd.read_csv(os.path.join(data_path, 'proximity_DDDEN.csv'), sep=";", index_col="ID")
DDDEN_df = pd.read_csv(os.path.join(data_path, 'DDDEN.csv'), sep=";", index_col="ID")

DDDEN_df.info()

DDDEN_df.head()

DDDEN_proximity.info()

DDDEN_proximity.shape

DDDEN_proximity.head()

# Columns to select and rename
dden_selected = DDDEN_df[[
    'Codering_3',
    'GemiddeldeWOZWaardeVanWoningen_39',
    'PercentageTweeOnderEenKapWoningEe_43',
    'PercentageVrijstaandeWoningEengezins_44',
    'BouwjaarAfgelopenTienJaar_52',
    'Omgevingsadressendichtheid_128',
    'MateVanStedelijkheid_127'
]].copy()

# Rename columns
dden_selected.rename(columns={
    'Codering_3': 'Coding',
    'GemiddeldeWOZWaardeVanWoningen_39': 'MeanWOZValueHome',
    'PercentageTweeOnderEenKapWoningEe_43': '%SemiDetached_SingleFamilyHome',
    'PercentageVrijstaandeWoningEengezins_44': '%DetachedHouseSingleFamilyHome',
    'BouwjaarAfgelopenTienJaar_52': 'BuiltLastTenYears',
    'Omgevingsadressendichtheid_128': 'PopulationDensity',
    'MateVanStedelijkheid_127': 'UrbanityLevel'
}, inplace=True)

# Replace '.' with NaN
dden_selected.replace('.', np.nan, inplace=True)

# Convert all columns except Coding to numeric
for col in dden_selected.columns:
    if col != 'Coding':
        dden_selected[col] = pd.to_numeric(dden_selected[col], errors='coerce')

# Drop rows where MeanWOZValueHome is missing (can't test without target)
dden_selected.dropna(subset=['MeanWOZValueHome'], inplace=True)

# Check the result
print("\n✅ Cleaned DDEN Shape:", dden_selected.shape)
display(dden_selected.head())

dden_selected.info()

# Clean DDDEN_proximity
ddden_proximity_clean = DDDEN_proximity[[
    'Codering_3',
    'AfstandTotTreinstationsTotaal_90',
    'AfstandTotBrandweerkazerne_114'
]].copy()

# Rename columns nicely
ddden_proximity_clean.rename(columns={
    'Codering_3': 'Coding',
    'AfstandTotTreinstationsTotaal_90': 'DistanceToTrainStation',
    'AfstandTotBrandweerkazerne_114': 'DistanceToFireStation'
}, inplace=True)

# Replace '.' with NaN
ddden_proximity_clean.replace('.', np.nan, inplace=True)

# Convert Distance columns to numeric
ddden_proximity_clean['DistanceToTrainStation'] = pd.to_numeric(ddden_proximity_clean['DistanceToTrainStation'], errors='coerce')
ddden_proximity_clean['DistanceToFireStation'] = pd.to_numeric(ddden_proximity_clean['DistanceToFireStation'], errors='coerce')

# Drop rows where important distances are missing
ddden_proximity_clean.dropna(subset=['DistanceToTrainStation', 'DistanceToFireStation'], inplace=True)

print("✅ Cleaned DDDEN Proximity Shape:", ddden_proximity_clean.shape)
ddden_proximity_clean.head()

# Merge on 'Coding'
merged_ddden = pd.merge(
    dden_selected,
    ddden_proximity_clean,
    on='Coding',
    how='inner'   # Only rows that exist in both
)

print("✅ Merged DDDEN Shape:", merged_ddden.shape)
merged_ddden.head()

# Drop rows where Coding starts with 'GM'
merged_ddden = merged_ddden[~merged_ddden['Coding'].str.startswith('GM')]

print("✅ Final Cleaned DDDEN Shape:", merged_ddden.shape)
merged_ddden.head()



selected_features = [
    '%SemiDetached_SingleFamilyHome',
    '%DetachedHouseSingleFamilyHome',
    'BuiltLastTenYears',
    'PopulationDensity',
    #'DistanceToGP',
    #'DistanceToSupermarket',
    'DistanceToTrainStation',
    'DistanceToFireStation',   # 🆕
    'UrbanityLevel',
    'AddressDensity'
]

# Select the features you used in the Tilburg physical model
selected_clean_features = [
    'BuiltLastTenYears',
    'PopulationDensity',
    'DistanceToTrainStation',
    'DistanceToFireStation',
    'UrbanityLevel'
]

# Prepare Breda X and y
X_ddden = merged_ddden[selected_clean_features].copy()
y_ddden_actual = merged_ddden['MeanWOZValueHome'].copy()

# If needed, impute missing (e.g. median imputer you trained before)
X_ddden_imputed = pd.DataFrame(imputer.transform(X_ddden), columns=X_ddden.columns)

# Predict
y_ddden_pred = xgb_small_clean.predict(X_ddden_imputed)

# Evaluate
print("\n📊 DDDEN Test Results:")
print(f"MAE: {mean_absolute_error(y_ddden_actual, y_ddden_pred):.2f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_ddden_actual, y_ddden_pred)):.2f}")
print(f"R² Score: {r2_score(y_ddden_actual, y_ddden_pred):.2f}")





"""Train and test an xgboost on the ddden dataset"""

# 📋 Define the selected features
selected_features_ddden = [
    '%SemiDetached_SingleFamilyHome',
    '%DetachedHouseSingleFamilyHome',
    'BuiltLastTenYears',
    'PopulationDensity',
    'UrbanityLevel',
    'DistanceToTrainStation',
    'DistanceToFireStation'
]

# 🧹 Prepare features and target
X_ddden = merged_ddden[selected_features_ddden].copy()
y_ddden = merged_ddden['MeanWOZValueHome'].copy()

# 🚿 Handle missing values (if any)
imputer_ddden = SimpleImputer(strategy='median')
X_ddden_imputed = pd.DataFrame(imputer_ddden.fit_transform(X_ddden), columns=X_ddden.columns)

# ✂️ Train/Test split (if you want internal validation)
X_train_ddden, X_test_ddden, y_train_ddden, y_test_ddden = train_test_split(
    X_ddden_imputed, y_ddden, test_size=0.2, random_state=42
)

# ⚡ Train XGBoost Model
xgb_ddden = xgb.XGBRegressor(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbosity=0
)

xgb_ddden.fit(X_train_ddden, y_train_ddden)

# 🎯 Predict on test set
y_pred_ddden = xgb_ddden.predict(X_test_ddden)

# 🧮 Evaluate model
mae = mean_absolute_error(y_test_ddden, y_pred_ddden)
rmse = np.sqrt(mean_squared_error(y_test_ddden, y_pred_ddden))
r2 = r2_score(y_test_ddden, y_pred_ddden)

print("\n📊 XGBoost Model on DDDEN Data:")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R² Score: {r2:.2f}")

import matplotlib.pyplot as plt

model_names = ['Baseline', 'Full Features', 'Physical Features', 'Breda', 'DDDEN']
rmse_scores = [52.82, 31.99, 58.91, 72.18, 100.08]
r2_scores = [0.84, 0.94, 0.80, 0.70, 0.14]

x = range(len(model_names))

plt.figure(figsize=(10, 5))
plt.plot(x, rmse_scores, marker='o', label='RMSE')
plt.plot(x, r2_scores, marker='o', label='R²', color='green')
plt.xticks(x, model_names, rotation=45)
plt.ylabel('Score')
plt.title('Model Performance: RMSE and R²')
plt.legend()
plt.tight_layout()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

model_names = ['Baseline', 'Full Features', 'Physical Features', 'Breda', 'DDDEN']
rmse_scores = [52.82, 31.99, 58.91, 72.18, 100.08]
r2_scores = [0.84, 0.94, 0.80, 0.70, 0.14]
mae_scores = [33.40, 21.94, 43.13, 52.33, 68.37]

x = range(len(model_names))

plt.figure(figsize=(12, 6))

# Plot all three metrics
plt.plot(x, rmse_scores, marker='o', label='RMSE', color='blue')
plt.plot(x, r2_scores, marker='o', label='R²', color='green')
plt.plot(x, mae_scores, marker='o', label='MAE', color='red')

plt.xticks(x, model_names, rotation=45)
plt.ylabel('Score')
plt.title('Model Performance Comparison')
plt.legend()
plt.tight_layout()
plt.grid(True, linestyle='--', alpha=0.7)

# Add value labels above each point
for i, (rmse, r2, mae) in enumerate(zip(rmse_scores, r2_scores, mae_scores)):
    plt.text(i, rmse+3, f'{rmse:.1f}', ha='center', color='blue')
    plt.text(i, r2+0.03, f'{r2:.2f}', ha='center', color='green')
    plt.text(i, mae+3, f'{mae:.1f}', ha='center', color='red')

plt.show()

# Bar plot version of Figure 15
metrics = ['MAE', 'RMSE', 'R²']
models = ['Baseline', 'Full Features', 'Physical Only']
values = [[33.40, 21.94, 43.13],
          [52.82, 31.99, 58.91],
          [0.84, 0.94, 0.80]]

fig, axes = plt.subplots(1, 3, figsize=(18, 6))

for i, (metric, vals) in enumerate(zip(metrics, values)):
    bars = axes[i].bar(models, vals, color=['#1f77b4', '#ff7f0e', '#2ca02c'])
    axes[i].set_title(metric, fontsize=14)
    axes[i].bar_label(bars, fmt='%.2f', padding=3)

plt.suptitle('Model Performance Comparison Across Metrics', y=1.05, fontsize=24)
plt.tight_layout()
plt.savefig('performance_comparison.pdf')